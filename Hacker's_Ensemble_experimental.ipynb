{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hacker's Ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-GHTDiX_Gb6o",
        "tWktnkPEyqzc",
        "Wh9YI_7cfujb",
        "_UR-PieU-Drb",
        "xJSTSiC62bsV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marvinmouroum/AccountingApp/blob/master/Hacker's_Ensemble_experimental.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation 6 Setup\n"
      ],
      "metadata": {
        "id": "n1kUgMNBqU7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount the Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVCHclqJqaqm",
        "outputId": "6c097498-34ba-4c0d-abe9-d200ec256539"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Part\n",
        "... Done by **Harry** & **Lauri**"
      ],
      "metadata": {
        "id": "JqBysgnC9MsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "-GHTDiX_Gb6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "\n",
        "def resize(im:np.array, desired_size:int):\n",
        "\n",
        "    old_size = im.size  # old_size[0] is in (width, height) format\n",
        "\n",
        "    ratio = float(desired_size)/max(old_size)\n",
        "    new_size = tuple([int(x*ratio) for x in old_size])\n",
        "    # use thumbnail() or resize() method to resize the input image\n",
        "\n",
        "    # thumbnail is a in-place operation\n",
        "\n",
        "    # im.thumbnail(new_size, Image.ANTIALIAS)\n",
        "\n",
        "    im = im.resize(new_size, Image.ANTIALIAS)\n",
        "    # create a new image and paste the resized on it\n",
        "\n",
        "    new_im = Image.new(\"RGB\", (desired_size, desired_size))\n",
        "    new_im.paste(im, ((desired_size-new_size[0])//2,\n",
        "                        (desired_size-new_size[1])//2))\n",
        "    return new_im\n",
        "\n",
        "\n",
        "def resize_all_images(path, desired_size):\n",
        "  for im_pth in glob(path+\"*\"):\n",
        "    im = Image.open(im_pth)\n",
        "    new_im = resize(im, desired_size)\n",
        "\n",
        "    new_im.save(\"/var/home/laatu/scenedata/train/resized/\"+im_pth.split(\"/\")[-1])"
      ],
      "metadata": {
        "id": "N2ck0GcqGd3d"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def new_label(old_size, new_size, labels):\n",
        "\n",
        "  largest = np.argmax(np.asarray([old_size[0], old_size[1]]))\n",
        "  smallest = np.argmin(np.asarray([old_size[0], old_size[1]]))\n",
        "\n",
        "  ratio = old_size[smallest]/ old_size[largest]\n",
        "  size_change_l = new_size[largest] / old_size[largest]\n",
        "  size_change_s = new_size[smallest] / old_size[smallest]\n",
        "\n",
        "  new_smallest = new_size[largest] * ratio\n",
        "\n",
        "  diff = new_size[smallest] - new_smallest\n",
        "\n",
        "  new_labels = []\n",
        "\n",
        "  for label in labels:\n",
        "    print(f\"old label: {label}\")\n",
        "\n",
        "    new_label = label\n",
        "\n",
        "    array = np.asarray(label)\n",
        "    array = array.reshape((4, 2))\n",
        "\n",
        "    array[:,1-smallest] *= size_change_l\n",
        "    array[:,1-largest] *= size_change_l\n",
        "\n",
        "    array[:,1-smallest] += diff/2\n",
        "\n",
        "    array[:,0] /= new_size[0]\n",
        "    array[:,1] /= new_size[1]\n",
        "\n",
        "    new_labels.append(array.flatten())\n",
        "\n",
        "    print(f\"new label: {array.flatten()}\")\n",
        "\n",
        "  return new_labels\n",
        "\n"
      ],
      "metadata": {
        "id": "1OUiTeaFYcqW"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "image = np.ones((200, 150, 3), dtype=np.float64)*100\n",
        "label = np.asarray([0., 0., \n",
        "         15., 0.,\n",
        "         15., 20.,\n",
        "         0., 20.]) * 100\n",
        "\n",
        "resized_image = tf.image.resize_with_pad(image,target_height=224, target_width=224)\n",
        "\n",
        "print(image.shape)\n",
        "print(resized_image.shape)\n",
        "\n",
        "image_rect = cv2.polylines(image,label.reshape(4,2).astype(np.int32).reshape((-1,1,2)),True,(0,255,255),20)\n",
        "\n",
        "cv2_imshow(image_rect)\n",
        "cv2_imshow(resized_image.numpy())\n",
        "\n",
        "d = new_label(image.shape, resized_image.shape, [label.tolist()])\n",
        "\n",
        "d[0] *= resized_image.shape[0]\n",
        "d[0] = d[0].astype(np.int32)\n",
        "\n",
        "print(d[0].reshape(4,2))\n",
        "\n",
        "resized_image_rect = cv2.polylines(resized_image.numpy(),d[0].reshape(4,2).reshape((-1,1,2)),True,(0,255,255),20)\n",
        "cv2_imshow(resized_image_rect)\n"
      ],
      "metadata": {
        "id": "fu3sA-VZHkPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader\n",
        "-- by Harry"
      ],
      "metadata": {
        "id": "tWktnkPEyqzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install patool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB6JZqNwWZih",
        "outputId": "67ce5716-c196-4a15-fc7f-85f243ed968f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: patool in /usr/local/lib/python3.7/dist-packages (1.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r train_data train_labels test_data test_labels"
      ],
      "metadata": {
        "id": "qNnq4WV-ZCDr",
        "outputId": "2c99c36c-f818-40de-e0d5-9493ef5398aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'train_data': No such file or directory\n",
            "rm: cannot remove 'train_labels': No such file or directory\n",
            "rm: cannot remove 'test_data': No such file or directory\n",
            "rm: cannot remove 'test_labels': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import glob \n",
        "import patoolib\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# NEED TO OVERWRITE THE TRAIN LABELS AFTER EACH ITER\n",
        "def overwrite_then_extract_archive(archive, verbosity=0, outdir=None, program=None):\n",
        "  if outdir:\n",
        "    if not os.listdir(outdir):\n",
        "      if os.path.exists(outdir):\n",
        "          shutil.rmtree(outdir)\n",
        "\n",
        "  return patoolib.extract_archive(archive, verbosity, outdir, program)\n",
        "\n",
        "# MONKEY PATCH\n",
        "patoolib.extract_archive = overwrite_then_extract_archive \n",
        "\n",
        "class DataLoader:\n",
        "  '''\n",
        "  A class to load the data and organise for training\n",
        "\n",
        "  Methods\n",
        "  -------\n",
        "  _mkdir: \n",
        "    make a directory\n",
        "  _unrar: \n",
        "    extract a rar\n",
        "  get_trainset_labels: \n",
        "    get the labels from training data\n",
        "  get_transet: \n",
        "    get the input data from training data\n",
        "  get_valset_labels: \n",
        "    get the labels from the validation data\n",
        "  get_valset: \n",
        "    get the input data from the validation data\n",
        "  '''\n",
        "\n",
        "  def __init__(self, \n",
        "               dest_input, \n",
        "               dest_labels,\n",
        "               dest_input_test,\n",
        "               dest_labels_test,\n",
        "               source_train =  'drive/MyDrive/Resources/train',\n",
        "               source_train_labels = 'drive/MyDrive/Resources/train/labels.rar',\n",
        "               source_val = 'drive/MyDrive/Resources/val',\n",
        "               source_val_labels = 'drive/MyDrive/Resources/val/labels.rar',\n",
        "               verbosity=1):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    dest : str \n",
        "      destination folder for data\n",
        "    verbosity : int [0, 1, 2]\n",
        "      display information for unrar package\n",
        "    '''\n",
        "\n",
        "    self.verbosity = verbosity\n",
        "    self.dest_input = dest_input\n",
        "    self.dest_labels = dest_labels\n",
        "    self.dest_input_test = dest_input_test\n",
        "    self.dest_labels_test = dest_labels_test\n",
        "    self._mkdir()\n",
        "    drive.mount('/content/drive')\n",
        "    self.train_path = source_train\n",
        "    self.train_labels_path = source_train_labels\n",
        "    self.train_files = glob.glob(self.train_path + '/*')[1:]\n",
        "\n",
        "    self.val_path = source_val\n",
        "    self.val_labels_path = source_val_labels\n",
        "    self.val_files = glob.glob(self.val_path + '/*')[1:]\n",
        "\n",
        "  def _mkdir(self):\n",
        "    ''' \n",
        "    Private make a directory if not made\n",
        "    '''\n",
        "    if os.path.exists(self.dest_input) == False:\n",
        "      os.mkdir(self.dest_input)\n",
        "    if os.path.exists(self.dest_labels) == False:\n",
        "      os.mkdir(self.dest_labels)\n",
        "    if os.path.exists(self.dest_input_test) == False:\n",
        "      os.mkdir(self.dest_input_test)\n",
        "    if os.path.exists(self.dest_labels_test) == False:\n",
        "      os.mkdir(self.dest_labels_test)\n",
        "      \n",
        "    print('directories made')\n",
        "\n",
        "  def _unrar(self, file_path, outdir):\n",
        "    '''\n",
        "    extract the rar files from the folder \n",
        "    Parameters\n",
        "    ----------\n",
        "    file path of rar file\n",
        "    Returns\n",
        "    -------\n",
        "    result if it worked\n",
        "    '''\n",
        "    return patoolib.extract_archive(file_path, \n",
        "                                  outdir=outdir,\n",
        "                                  verbosity=self.verbosity)\n",
        "  \n",
        "  def get_trainset_labels(self):\n",
        "    '''\n",
        "    Get the labels for the trainset\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    result if it worked\n",
        "    '''\n",
        "    return self._unrar(self.train_labels_path, self.dest_labels)\n",
        "\n",
        "  def get_trainset(self):\n",
        "    '''\n",
        "    Generator to stream each rar file and serve\n",
        "    images into designated file path\n",
        "    \n",
        "    returns\n",
        "    -------\n",
        "    iterator for extracting rar\n",
        "    '''\n",
        "    for file in self.train_files:\n",
        "      yield self._unrar(file, self.dest_input)\n",
        "  \n",
        "  def get_valset_labels(self):\n",
        "    '''\n",
        "    Same thing as get_trainset_labels\n",
        "    '''\n",
        "    return self._unrar(self.val_labels_path, self.dest_labels_test)\n",
        "\n",
        "  def get_valset(self):\n",
        "    ''' \n",
        "    Same thing as get_transet\n",
        "    '''\n",
        "    for file in self.val_files:\n",
        "      yield self._unrar(file, self.dest_input_test)\n"
      ],
      "metadata": {
        "id": "IBMynsRZO7Jt"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DataLoader('train_data', 'train_labels',\n",
        "                    'test_data', 'test_labels')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy_X_SREWTp_",
        "outputId": "57a3bc80-575b-4cce-ce36-babf11a37d37"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "directories made\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "8uoOb1De3LZJ",
        "outputId": "496b6ec6-69eb-4c72-b784-8a527f60d884",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive\t     test_data\t  train_data\tunzipped_images\n",
            "sample_data  test_labels  train_labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# just use for training. When running for val, run the DataLoader('val_input',\n",
        "# 'val_labels'). Don't forget mkdirb, can't be f'ed to do it in class. Maybe l8r\n",
        "# this takes so long\n",
        "\n",
        "# DON'T FORGET, THE LABELS ARE FOR THE ENTIRE TRAIN SET, SO INDEX\n",
        "res = loader.get_trainset_labels()\n",
        "if res:\n",
        "  for _ in loader.get_trainset():\n",
        "    loader.prepare_directory('train')\n",
        "    print(\"do training\")"
      ],
      "metadata": {
        "id": "bJIQ9WI_X6Te",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "faee7ebe-8c29-4e4f-8c7f-9f2d1557c074"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-82ff20c45319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# DON'T FORGET, THE LABELS ARE FOR THE ENTIRE TRAIN SET, SO INDEX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trainset_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trainset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-111126edf442>\u001b[0m in \u001b[0;36mget_trainset_labels\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0mworked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     '''\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unrar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_labels_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_trainset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-111126edf442>\u001b[0m in \u001b[0;36m_unrar\u001b[0;34m(self, file_path, outdir)\u001b[0m\n\u001b[1;32m     98\u001b[0m     return patoolib.extract_archive(file_path, \n\u001b[1;32m     99\u001b[0m                                   \u001b[0moutdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                                   verbosity=self.verbosity)\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_trainset_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-111126edf442>\u001b[0m in \u001b[0;36moverwrite_then_extract_archive\u001b[0;34m(archive, verbosity, outdir, program)\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mpatoolib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# MONKEY PATCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-111126edf442>\u001b[0m in \u001b[0;36moverwrite_then_extract_archive\u001b[0;34m(archive, verbosity, outdir, program)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moverwrite_then_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moutdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_labels'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a dataset"
      ],
      "metadata": {
        "id": "Wh9YI_7cfujb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "MmwoQY8ygFRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PIL.Image.open('train_input/images/00000.png')"
      ],
      "metadata": {
        "id": "1wScHPItgMhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180"
      ],
      "metadata": {
        "id": "nZMeg2Dgg9JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  'train_data',\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  labels=''\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "metadata": {
        "id": "oXKLnd9_fsMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset for Mask"
      ],
      "metadata": {
        "id": "Z2FEd27LDVxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2jh9-GwYDxZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_validation_data():\n",
        "\n",
        "  !mkdir unzipped_images\n",
        "  !mkdir unzipped_images/train\n",
        "  !mkdir unzipped_images/val\n",
        "\n",
        "  !unrar x drive/MyDrive/Resources/val/images.part7.rar unzipped_images/val\n",
        "\n",
        "  !ls unzipped_images/val/images\n",
        "\n",
        "  return \"unzipped_images/val/images\""
      ],
      "metadata": {
        "id": "ryKOLYwOQ-g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls unzipped_images/val/images"
      ],
      "metadata": {
        "id": "ZJhCsDMpp94P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_validation_data()"
      ],
      "metadata": {
        "id": "rHr_pdrXASts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Labels Need to changed into Segmentation Masks\n",
        "\n",
        "*Harry's Github code copy paste*"
      ],
      "metadata": {
        "id": "CDA--OYzhtdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "from PIL import Image, ImageDraw\n",
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "def data2mask(polygon: list, out_img_size: int) -> np.array: \n",
        "    '''\n",
        "    Returns a mask from an 8 point polygon.\n",
        "\n",
        "    :param polygon<list[int]>: list of 8 points, can be tuples.\n",
        "    :param out_img_size<int>: creates an output image size (h == w)\n",
        "    '''\n",
        "\n",
        "    img = Image.new('L', (out_img_size, out_img_size), 0)\n",
        "    ImageDraw.Draw(img).polygon(polygon, outline=1, fill=1)\n",
        "    return np.array(img)\n",
        "\n",
        "\n",
        "def file2data(filename: str, orig_img_size: int, out_img_size: int) -> list:\n",
        "    '''\n",
        "    Returns normalised data from the text file \n",
        "\n",
        "    :param filename<str>: source of the labels txt data\n",
        "    :param orig_img_size<int>: the original size of the image (assuming x == y)\n",
        "    :param out_img_size<int>: the size of the output image (x == y)\n",
        "    '''\n",
        "\n",
        "    with open(filename, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        data = [list(\n",
        "                map(lambda x: int((x / orig_img_size)*(out_img_size)),\n",
        "                    list(map(int, line.split('\\n')[0].split(','))))) for line in lines]\n",
        "\n",
        "    file.close()\n",
        "\n",
        "    return data\n",
        "\n",
        "def file2segmask(filename: str, orig_img_size:int, out_img_size:int, CONVEX_HULL=True) -> np.array:\n",
        "\n",
        "    '''\n",
        "    Runs the mask segmentation over a file: labels.txt.\n",
        "    :param filename<str>: the filename within labels (00000.txt for example)\n",
        "    :param orig_img_size<int>: size of the original image (assuming h==w)\n",
        "    :param out_img_size<int>: size of the output image for training (h==w)\n",
        "    '''\n",
        "\n",
        "    # init\n",
        "    out_mat: list = []\n",
        "\n",
        "    # run\n",
        "    data = file2data(filename, orig_img_size, out_img_size)\n",
        "\n",
        "    if CONVEX_HULL:\n",
        "        for row in data:\n",
        "            row = convhull(row)\n",
        "            out_mat.append(data2mask(row, out_img_size))\n",
        "\n",
        "    else:\n",
        "        for row in data:\n",
        "            out_mat.append(data2mask(row, out_img_size))\n",
        "\n",
        "    # return\n",
        "    return np.array(out_mat)\n",
        "\n",
        "def convhull(data: np.array) -> np.array:\n",
        "    '''\n",
        "    Creates the convex hull of a row of data (8 points) from the labels\n",
        "    :param data<np.array>: the array of data (8 point array)\n",
        "    :return: convex hull of the points for a simpler polygon\n",
        "    '''\n",
        "\n",
        "    out_data: list = []\n",
        "\n",
        "    for previous, current in zip(data, data[1:]):\n",
        "        out_data.append((previous, current))\n",
        "\n",
        "    hull = ConvexHull(out_data)\n",
        "    out_data = hull.points[np.unique(hull.simplices)]\n",
        "\n",
        "    return out_data\n",
        "\n",
        "\n",
        "# TO RUN:\n",
        "# Specify the original img size, output img size, the files where the labels\n",
        "# are located. \n",
        "# Specify if convex hull is ON or not (selecting the convex hull of labels, for cleaner labelling)\n",
        "# if __name__ == \"__main__\":\n",
        "\n",
        "#     # init\n",
        "#     orig_img_size = 3000\n",
        "#     out_img_size = 200\n",
        "#     filename = '../Ultrahack/labels/00000.txt'\n",
        "#     CONVEX_HULL = True\n",
        "\n",
        "#     # run\n",
        "#     seg_labels = file2segmask(filename, orig_img_size, out_img_size, CONVEX_HULL=CONVEX_HULL)\n",
        "\n",
        "#     # print\n",
        "#     print(seg_labels)"
      ],
      "metadata": {
        "id": "OHj7SjdltVyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 256\n",
        "BATCH_SIZE = 4\n",
        "NUM_CLASSES = 1\n",
        "DATA_DIR = \"./instance-level_human_parsing/instance-level_human_parsing\"\n",
        "NUM_TRAIN_IMAGES = 1000\n",
        "NUM_VAL_IMAGES = 50"
      ],
      "metadata": {
        "id": "0GBtHV7BGleY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[:NUM_TRAIN_IMAGES]\n",
        "train_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[:NUM_TRAIN_IMAGES]\n",
        "val_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[\n",
        "    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n",
        "]\n",
        "val_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[\n",
        "    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n",
        "]\n",
        "\n",
        "\n",
        "def read_image(image_path, mask=False):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    if mask:\n",
        "        image = tf.image.decode_png(image, channels=1)\n",
        "        image.set_shape([None, None, 1])\n",
        "        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "    else:\n",
        "        image = tf.image.decode_png(image, channels=3)\n",
        "        image.set_shape([None, None, 3])\n",
        "        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "        image = image / 127.5 - 1\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_data(image_list, mask_list):\n",
        "    image = read_image(image_list)\n",
        "    mask = read_image(mask_list, mask=True)\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "def data_generator(image_list, mask_list):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n",
        "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_dataset = data_generator(train_images, train_masks)\n",
        "val_dataset = data_generator(val_images, val_masks)\n",
        "\n",
        "print(\"Train Dataset:\", train_dataset)\n",
        "print(\"Val Dataset:\", val_dataset)"
      ],
      "metadata": {
        "id": "c_gGOO9pDsec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "id": "DMDwrr1JiUsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i3UYcYSOiylL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deeplab Part\n",
        "...Done by **Marvin**"
      ],
      "metadata": {
        "id": "1LtDaSDv9YqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "an9eEpce9daL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-efiFA6zGp4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition\n",
        "\n",
        "deeplabv3+"
      ],
      "metadata": {
        "id": "HQNPvSxBINCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convolution_block(\n",
        "    block_input,\n",
        "    num_filters=256,\n",
        "    kernel_size=3,\n",
        "    dilation_rate=1,\n",
        "    padding=\"same\",\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=dilation_rate,\n",
        "        padding=\"same\",\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=keras.initializers.HeNormal(),\n",
        "    )(block_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def DilatedSpatialPyramidPooling(dspp_input):\n",
        "    dims = dspp_input.shape\n",
        "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
        "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
        "    out_pool = layers.UpSampling2D(\n",
        "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "\n",
        "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
        "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
        "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
        "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
        "    output = convolution_block(x, kernel_size=1)\n",
        "    return output"
      ],
      "metadata": {
        "id": "fdloF_V4GpLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def set_backbone(name, model_input):\n",
        "\n",
        "  backbone = None\n",
        "  last_layer = None\n",
        "  residual_layer = None\n",
        "  devider = 1\n",
        "\n",
        "  if name == \"resnet\":\n",
        "    resnet50 = keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_tensor=model_input)\n",
        "    backbone = resnet50\n",
        "    last_layer = \"conv4_block6_2_relu\"\n",
        "    residual_layer = \"conv2_block3_2_relu\"\n",
        "    devider = 4\n",
        "  \n",
        "  if name == \"xception\":\n",
        "    xception = keras.applications.Xception(weights=\"imagenet\", include_top=False, input_tensor=model_input)\n",
        "    backbone = xception\n",
        "    last_layer = \"block13_sepconv1_act\"\n",
        "    residual_layer = \"block4_sepconv1_act\"\n",
        "    devider = 8\n",
        "\n",
        "  if name == \"mobilenet\":\n",
        "    mobilenet = tf.keras.applications.MobileNetV2(weights=\"imagenet\", include_top=True, input_tensor=model_input)\n",
        "    backbone = mobilenet\n",
        "    last_layer = \"block_15_depthwise_relu\"\n",
        "    residual_layer = \"block_2_depthwise_relu\"\n",
        "    devider = 4\n",
        "\n",
        "  return backbone, last_layer, residual_layer, devider"
      ],
      "metadata": {
        "id": "QEbfMGfAaC-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = keras.Input(shape=(256, 256, 3))\n",
        "backbone, last_layer, residual_layer, n = set_backbone(name=\"mobilenet\",model_input=model_input)"
      ],
      "metadata": {
        "id": "ztw4-mO7e2_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone.summary()"
      ],
      "metadata": {
        "id": "fa16a3jhL13z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DeeplabV3Plus(image_size, num_classes, model_input=None):\n",
        "    \n",
        "    if model_input is None:\n",
        "      model_input = keras.Input(shape=(image_size, image_size, 3))\n",
        "\n",
        "    backbone, last_layer, residual_layer, devider = set_backbone(name=\"mobilenet\",model_input=model_input)\n",
        "\n",
        "    x = backbone.get_layer(last_layer).output\n",
        "    x = DilatedSpatialPyramidPooling(x)\n",
        "\n",
        "    input_a = layers.UpSampling2D(\n",
        "        size=(image_size // devider // x.shape[1], image_size // devider // x.shape[2] ),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    input_b = backbone.get_layer(residual_layer).output\n",
        "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
        "\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
        "    x = convolution_block(x)\n",
        "    x = convolution_block(x)\n",
        "    x = layers.UpSampling2D(\n",
        "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
        "    return keras.Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "\n",
        "model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "YyLdfpGYG07N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "anyy5hueCv6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = loader.get_trainset()\n",
        "val_dataset = loader.get_valset()\n",
        "\n",
        "print(\"Train Dataset:\", train_dataset)\n",
        "print(\"Val Dataset:\", val_dataset)"
      ],
      "metadata": {
        "id": "wQaHc-qFEZWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=loss,\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=25)\n",
        "\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.title(\"Training Loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"accuracy\"])\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title(\"Validation Loss\")\n",
        "plt.ylabel(\"val_loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.ylabel(\"val_accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qEm1_bCVC0WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detection Part\n",
        ".. Done by **Jonny**"
      ],
      "metadata": {
        "id": "_UR-PieU-Drb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Includes:\n",
        "* Function to compute the IoU similarity for axis-aligned, rectangular, 2D bounding boxes\n",
        "* Function for coordinate conversion for axis-aligned, rectangular, 2D bounding boxes\n",
        "\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "\n",
        "def convert_coordinates(tensor, start_index, conversion, border_pixels='half'):\n",
        "    '''\n",
        "    Convert coordinates for axis-aligned 2D boxes between two coordinate formats.\n",
        "\n",
        "    Creates a copy of `tensor`, i.e. does not operate in place. Currently there are\n",
        "    three supported coordinate formats that can be converted from and to each other:\n",
        "        1) (xmin, xmax, ymin, ymax) - the 'minmax' format\n",
        "        2) (xmin, ymin, xmax, ymax) - the 'corners' format\n",
        "        2) (cx, cy, w, h) - the 'centroids' format\n",
        "\n",
        "    Arguments:\n",
        "        tensor (array): A Numpy nD array containing the four consecutive coordinates\n",
        "            to be converted somewhere in the last axis.\n",
        "        start_index (int): The index of the first coordinate in the last axis of `tensor`.\n",
        "        conversion (str, optional): The conversion direction. Can be 'minmax2centroids',\n",
        "            'centroids2minmax', 'corners2centroids', 'centroids2corners', 'minmax2corners',\n",
        "            or 'corners2minmax'.\n",
        "        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
        "            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
        "            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
        "            If 'half', then one of each of the two horizontal and vertical borders belong\n",
        "            to the boxex, but not the other.\n",
        "\n",
        "    Returns:\n",
        "        A Numpy nD array, a copy of the input tensor with the converted coordinates\n",
        "        in place of the original coordinates and the unaltered elements of the original\n",
        "        tensor elsewhere.\n",
        "    '''\n",
        "    if border_pixels == 'half':\n",
        "        d = 0\n",
        "    elif border_pixels == 'include':\n",
        "        d = 1\n",
        "    elif border_pixels == 'exclude':\n",
        "        d = -1\n",
        "\n",
        "    ind = start_index\n",
        "    tensor1 = np.copy(tensor).astype(np.float)\n",
        "    if conversion == 'minmax2centroids':\n",
        "        tensor1[..., ind] = (tensor[..., ind] + tensor[..., ind+1]) / 2.0 # Set cx\n",
        "        tensor1[..., ind+1] = (tensor[..., ind+2] + tensor[..., ind+3]) / 2.0 # Set cy\n",
        "        tensor1[..., ind+2] = tensor[..., ind+1] - tensor[..., ind] + d # Set w\n",
        "        tensor1[..., ind+3] = tensor[..., ind+3] - tensor[..., ind+2] + d # Set h\n",
        "    elif conversion == 'centroids2minmax':\n",
        "        tensor1[..., ind] = tensor[..., ind] - tensor[..., ind+2] / 2.0 # Set xmin\n",
        "        tensor1[..., ind+1] = tensor[..., ind] + tensor[..., ind+2] / 2.0 # Set xmax\n",
        "        tensor1[..., ind+2] = tensor[..., ind+1] - tensor[..., ind+3] / 2.0 # Set ymin\n",
        "        tensor1[..., ind+3] = tensor[..., ind+1] + tensor[..., ind+3] / 2.0 # Set ymax\n",
        "    elif conversion == 'corners2centroids':\n",
        "        tensor1[..., ind] = (tensor[..., ind] + tensor[..., ind+2]) / 2.0 # Set cx\n",
        "        tensor1[..., ind+1] = (tensor[..., ind+1] + tensor[..., ind+3]) / 2.0 # Set cy\n",
        "        tensor1[..., ind+2] = tensor[..., ind+2] - tensor[..., ind] + d # Set w\n",
        "        tensor1[..., ind+3] = tensor[..., ind+3] - tensor[..., ind+1] + d # Set h\n",
        "    elif conversion == 'centroids2corners':\n",
        "        tensor1[..., ind] = tensor[..., ind] - tensor[..., ind+2] / 2.0 # Set xmin\n",
        "        tensor1[..., ind+1] = tensor[..., ind+1] - tensor[..., ind+3] / 2.0 # Set ymin\n",
        "        tensor1[..., ind+2] = tensor[..., ind] + tensor[..., ind+2] / 2.0 # Set xmax\n",
        "        tensor1[..., ind+3] = tensor[..., ind+1] + tensor[..., ind+3] / 2.0 # Set ymax\n",
        "    elif (conversion == 'minmax2corners') or (conversion == 'corners2minmax'):\n",
        "        tensor1[..., ind+1] = tensor[..., ind+2]\n",
        "        tensor1[..., ind+2] = tensor[..., ind+1]\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected conversion value. Supported values are 'minmax2centroids', 'centroids2minmax', 'corners2centroids', 'centroids2corners', 'minmax2corners', and 'corners2minmax'.\")\n",
        "\n",
        "    return tensor1\n",
        "\n",
        "def convert_coordinates2(tensor, start_index, conversion):\n",
        "    '''\n",
        "    A matrix multiplication implementation of `convert_coordinates()`.\n",
        "    Supports only conversion between the 'centroids' and 'minmax' formats.\n",
        "\n",
        "    This function is marginally slower on average than `convert_coordinates()`,\n",
        "    probably because it involves more (unnecessary) arithmetic operations (unnecessary\n",
        "    because the two matrices are sparse).\n",
        "\n",
        "    For details please refer to the documentation of `convert_coordinates()`.\n",
        "    '''\n",
        "    ind = start_index\n",
        "    tensor1 = np.copy(tensor).astype(np.float)\n",
        "    if conversion == 'minmax2centroids':\n",
        "        M = np.array([[0.5, 0. , -1.,  0.],\n",
        "                      [0.5, 0. ,  1.,  0.],\n",
        "                      [0. , 0.5,  0., -1.],\n",
        "                      [0. , 0.5,  0.,  1.]])\n",
        "        tensor1[..., ind:ind+4] = np.dot(tensor1[..., ind:ind+4], M)\n",
        "    elif conversion == 'centroids2minmax':\n",
        "        M = np.array([[ 1. , 1. ,  0. , 0. ],\n",
        "                      [ 0. , 0. ,  1. , 1. ],\n",
        "                      [-0.5, 0.5,  0. , 0. ],\n",
        "                      [ 0. , 0. , -0.5, 0.5]]) # The multiplicative inverse of the matrix above\n",
        "        tensor1[..., ind:ind+4] = np.dot(tensor1[..., ind:ind+4], M)\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected conversion value. Supported values are 'minmax2centroids' and 'centroids2minmax'.\")\n",
        "\n",
        "    return tensor1\n",
        "\n",
        "def intersection_area(boxes1, boxes2, coords='centroids', mode='outer_product', border_pixels='half'):\n",
        "    '''\n",
        "    Computes the intersection areas of two sets of axis-aligned 2D rectangular boxes.\n",
        "\n",
        "    Let `boxes1` and `boxes2` contain `m` and `n` boxes, respectively.\n",
        "\n",
        "    In 'outer_product' mode, returns an `(m,n)` matrix with the intersection areas for all possible\n",
        "    combinations of the boxes in `boxes1` and `boxes2`.\n",
        "\n",
        "    In 'element-wise' mode, `m` and `n` must be broadcast-compatible. Refer to the explanation\n",
        "    of the `mode` argument for details.\n",
        "\n",
        "    Arguments:\n",
        "        boxes1 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates for one box in the\n",
        "            format specified by `coords` or a 2D Numpy array of shape `(m, 4)` containing the coordinates for `m` boxes.\n",
        "            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes2`.\n",
        "        boxes2 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates for one box in the\n",
        "            format specified by `coords` or a 2D Numpy array of shape `(n, 4)` containing the coordinates for `n` boxes.\n",
        "            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes1`.\n",
        "        coords (str, optional): The coordinate format in the input arrays. Can be either 'centroids' for the format\n",
        "            `(cx, cy, w, h)`, 'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format\n",
        "            `(xmin, ymin, xmax, ymax)`.\n",
        "        mode (str, optional): Can be one of 'outer_product' and 'element-wise'. In 'outer_product' mode, returns an\n",
        "            `(m,n)` matrix with the intersection areas for all possible combinations of the `m` boxes in `boxes1` with the\n",
        "            `n` boxes in `boxes2`. In 'element-wise' mode, returns a 1D array and the shapes of `boxes1` and `boxes2`\n",
        "            must be boadcast-compatible. If both `boxes1` and `boxes2` have `m` boxes, then this returns an array of\n",
        "            length `m` where the i-th position contains the intersection area of `boxes1[i]` with `boxes2[i]`.\n",
        "        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
        "            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
        "            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
        "            If 'half', then one of each of the two horizontal and vertical borders belong\n",
        "            to the boxex, but not the other.\n",
        "\n",
        "    Returns:\n",
        "        A 1D or 2D Numpy array (refer to the `mode` argument for details) of dtype float containing values with\n",
        "        the intersection areas of the boxes in `boxes1` and `boxes2`.\n",
        "    '''\n",
        "\n",
        "    # Make sure the boxes have the right shapes.\n",
        "    if boxes1.ndim > 2: raise ValueError(\"boxes1 must have rank either 1 or 2, but has rank {}.\".format(boxes1.ndim))\n",
        "    if boxes2.ndim > 2: raise ValueError(\"boxes2 must have rank either 1 or 2, but has rank {}.\".format(boxes2.ndim))\n",
        "\n",
        "    if boxes1.ndim == 1: boxes1 = np.expand_dims(boxes1, axis=0)\n",
        "    if boxes2.ndim == 1: boxes2 = np.expand_dims(boxes2, axis=0)\n",
        "\n",
        "    if not (boxes1.shape[1] == boxes2.shape[1] == 4): raise ValueError(\"All boxes must consist of 4 coordinates, but the boxes in `boxes1` and `boxes2` have {} and {} coordinates, respectively.\".format(boxes1.shape[1], boxes2.shape[1]))\n",
        "    if not mode in {'outer_product', 'element-wise'}: raise ValueError(\"`mode` must be one of 'outer_product' and 'element-wise', but got '{}'.\",format(mode))\n",
        "\n",
        "    # Convert the coordinates if necessary.\n",
        "    if coords == 'centroids':\n",
        "        boxes1 = convert_coordinates(boxes1, start_index=0, conversion='centroids2corners')\n",
        "        boxes2 = convert_coordinates(boxes2, start_index=0, conversion='centroids2corners')\n",
        "        coords = 'corners'\n",
        "    elif not (coords in {'minmax', 'corners'}):\n",
        "        raise ValueError(\"Unexpected value for `coords`. Supported values are 'minmax', 'corners' and 'centroids'.\")\n",
        "\n",
        "    m = boxes1.shape[0] # The number of boxes in `boxes1`\n",
        "    n = boxes2.shape[0] # The number of boxes in `boxes2`\n",
        "\n",
        "    # Set the correct coordinate indices for the respective formats.\n",
        "    if coords == 'corners':\n",
        "        xmin = 0\n",
        "        ymin = 1\n",
        "        xmax = 2\n",
        "        ymax = 3\n",
        "    elif coords == 'minmax':\n",
        "        xmin = 0\n",
        "        xmax = 1\n",
        "        ymin = 2\n",
        "        ymax = 3\n",
        "\n",
        "    if border_pixels == 'half':\n",
        "        d = 0\n",
        "    elif border_pixels == 'include':\n",
        "        d = 1 # If border pixels are supposed to belong to the bounding boxes, we have to add one pixel to any difference `xmax - xmin` or `ymax - ymin`.\n",
        "    elif border_pixels == 'exclude':\n",
        "        d = -1 # If border pixels are not supposed to belong to the bounding boxes, we have to subtract one pixel from any difference `xmax - xmin` or `ymax - ymin`.\n",
        "\n",
        "    # Compute the intersection areas.\n",
        "\n",
        "    if mode == 'outer_product':\n",
        "\n",
        "        # For all possible box combinations, get the greater xmin and ymin values.\n",
        "        # This is a tensor of shape (m,n,2).\n",
        "        min_xy = np.maximum(np.tile(np.expand_dims(boxes1[:,[xmin,ymin]], axis=1), reps=(1, n, 1)),\n",
        "                            np.tile(np.expand_dims(boxes2[:,[xmin,ymin]], axis=0), reps=(m, 1, 1)))\n",
        "\n",
        "        # For all possible box combinations, get the smaller xmax and ymax values.\n",
        "        # This is a tensor of shape (m,n,2).\n",
        "        max_xy = np.minimum(np.tile(np.expand_dims(boxes1[:,[xmax,ymax]], axis=1), reps=(1, n, 1)),\n",
        "                            np.tile(np.expand_dims(boxes2[:,[xmax,ymax]], axis=0), reps=(m, 1, 1)))\n",
        "\n",
        "        # Compute the side lengths of the intersection rectangles.\n",
        "        side_lengths = np.maximum(0, max_xy - min_xy + d)\n",
        "\n",
        "        return side_lengths[:,:,0] * side_lengths[:,:,1]\n",
        "\n",
        "    elif mode == 'element-wise':\n",
        "\n",
        "        min_xy = np.maximum(boxes1[:,[xmin,ymin]], boxes2[:,[xmin,ymin]])\n",
        "        max_xy = np.minimum(boxes1[:,[xmax,ymax]], boxes2[:,[xmax,ymax]])\n",
        "\n",
        "        # Compute the side lengths of the intersection rectangles.\n",
        "        side_lengths = np.maximum(0, max_xy - min_xy + d)\n",
        "\n",
        "        return side_lengths[:,0] * side_lengths[:,1]\n",
        "\n",
        "def intersection_area_(boxes1, boxes2, coords='corners', mode='outer_product', border_pixels='half'):\n",
        "    '''\n",
        "    The same as 'intersection_area()' but for internal use, i.e. without all the safety checks.\n",
        "    '''\n",
        "\n",
        "    m = boxes1.shape[0] # The number of boxes in `boxes1`\n",
        "    n = boxes2.shape[0] # The number of boxes in `boxes2`\n",
        "\n",
        "    # Set the correct coordinate indices for the respective formats.\n",
        "    if coords == 'corners':\n",
        "        xmin = 0\n",
        "        ymin = 1\n",
        "        xmax = 2\n",
        "        ymax = 3\n",
        "    elif coords == 'minmax':\n",
        "        xmin = 0\n",
        "        xmax = 1\n",
        "        ymin = 2\n",
        "        ymax = 3\n",
        "\n",
        "    if border_pixels == 'half':\n",
        "        d = 0\n",
        "    elif border_pixels == 'include':\n",
        "        d = 1 # If border pixels are supposed to belong to the bounding boxes, we have to add one pixel to any difference `xmax - xmin` or `ymax - ymin`.\n",
        "    elif border_pixels == 'exclude':\n",
        "        d = -1 # If border pixels are not supposed to belong to the bounding boxes, we have to subtract one pixel from any difference `xmax - xmin` or `ymax - ymin`.\n",
        "\n",
        "    # Compute the intersection areas.\n",
        "\n",
        "    if mode == 'outer_product':\n",
        "\n",
        "        # For all possible box combinations, get the greater xmin and ymin values.\n",
        "        # This is a tensor of shape (m,n,2).\n",
        "        min_xy = np.maximum(np.tile(np.expand_dims(boxes1[:,[xmin,ymin]], axis=1), reps=(1, n, 1)),\n",
        "                            np.tile(np.expand_dims(boxes2[:,[xmin,ymin]], axis=0), reps=(m, 1, 1)))\n",
        "\n",
        "        # For all possible box combinations, get the smaller xmax and ymax values.\n",
        "        # This is a tensor of shape (m,n,2).\n",
        "        max_xy = np.minimum(np.tile(np.expand_dims(boxes1[:,[xmax,ymax]], axis=1), reps=(1, n, 1)),\n",
        "                            np.tile(np.expand_dims(boxes2[:,[xmax,ymax]], axis=0), reps=(m, 1, 1)))\n",
        "\n",
        "        # Compute the side lengths of the intersection rectangles.\n",
        "        side_lengths = np.maximum(0, max_xy - min_xy + d)\n",
        "\n",
        "        return side_lengths[:,:,0] * side_lengths[:,:,1]\n",
        "\n",
        "    elif mode == 'element-wise':\n",
        "\n",
        "        min_xy = np.maximum(boxes1[:,[xmin,ymin]], boxes2[:,[xmin,ymin]])\n",
        "        max_xy = np.minimum(boxes1[:,[xmax,ymax]], boxes2[:,[xmax,ymax]])\n",
        "\n",
        "        # Compute the side lengths of the intersection rectangles.\n",
        "        side_lengths = np.maximum(0, max_xy - min_xy + d)\n",
        "\n",
        "        return side_lengths[:,0] * side_lengths[:,1]\n",
        "\n",
        "\n",
        "def iou(boxes1, boxes2, coords='centroids', mode='outer_product', border_pixels='half'):\n",
        "    '''\n",
        "    Computes the intersection-over-union similarity (also known as Jaccard similarity)\n",
        "    of two sets of axis-aligned 2D rectangular boxes.\n",
        "\n",
        "    Let `boxes1` and `boxes2` contain `m` and `n` boxes, respectively.\n",
        "\n",
        "    In 'outer_product' mode, returns an `(m,n)` matrix with the IoUs for all possible\n",
        "    combinations of the boxes in `boxes1` and `boxes2`.\n",
        "\n",
        "    In 'element-wise' mode, `m` and `n` must be broadcast-compatible. Refer to the explanation\n",
        "    of the `mode` argument for details.\n",
        "\n",
        "    Arguments:\n",
        "        boxes1 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates for one box in the\n",
        "            format specified by `coords` or a 2D Numpy array of shape `(m, 4)` containing the coordinates for `m` boxes.\n",
        "            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes2`.\n",
        "        boxes2 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates for one box in the\n",
        "            format specified by `coords` or a 2D Numpy array of shape `(n, 4)` containing the coordinates for `n` boxes.\n",
        "            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes1`.\n",
        "        coords (str, optional): The coordinate format in the input arrays. Can be either 'centroids' for the format\n",
        "            `(cx, cy, w, h)`, 'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format\n",
        "            `(xmin, ymin, xmax, ymax)`.\n",
        "        mode (str, optional): Can be one of 'outer_product' and 'element-wise'. In 'outer_product' mode, returns an\n",
        "            `(m,n)` matrix with the IoU overlaps for all possible combinations of the `m` boxes in `boxes1` with the\n",
        "            `n` boxes in `boxes2`. In 'element-wise' mode, returns a 1D array and the shapes of `boxes1` and `boxes2`\n",
        "            must be boadcast-compatible. If both `boxes1` and `boxes2` have `m` boxes, then this returns an array of\n",
        "            length `m` where the i-th position contains the IoU overlap of `boxes1[i]` with `boxes2[i]`.\n",
        "        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
        "            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
        "            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
        "            If 'half', then one of each of the two horizontal and vertical borders belong\n",
        "            to the boxex, but not the other.\n",
        "\n",
        "    Returns:\n",
        "        A 1D or 2D Numpy array (refer to the `mode` argument for details) of dtype float containing values in [0,1],\n",
        "        the Jaccard similarity of the boxes in `boxes1` and `boxes2`. 0 means there is no overlap between two given\n",
        "        boxes, 1 means their coordinates are identical.\n",
        "    '''\n",
        "\n",
        "    # Make sure the boxes have the right shapes.\n",
        "    if boxes1.ndim > 2: raise ValueError(\"boxes1 must have rank either 1 or 2, but has rank {}.\".format(boxes1.ndim))\n",
        "    if boxes2.ndim > 2: raise ValueError(\"boxes2 must have rank either 1 or 2, but has rank {}.\".format(boxes2.ndim))\n",
        "\n",
        "    if boxes1.ndim == 1: boxes1 = np.expand_dims(boxes1, axis=0)\n",
        "    if boxes2.ndim == 1: boxes2 = np.expand_dims(boxes2, axis=0)\n",
        "\n",
        "    if not (boxes1.shape[1] == boxes2.shape[1] == 4): raise ValueError(\"All boxes must consist of 4 coordinates, but the boxes in `boxes1` and `boxes2` have {} and {} coordinates, respectively.\".format(boxes1.shape[1], boxes2.shape[1]))\n",
        "    if not mode in {'outer_product', 'element-wise'}: raise ValueError(\"`mode` must be one of 'outer_product' and 'element-wise', but got '{}'.\".format(mode))\n",
        "\n",
        "    # Convert the coordinates if necessary.\n",
        "    if coords == 'centroids':\n",
        "        boxes1 = convert_coordinates(boxes1, start_index=0, conversion='centroids2corners')\n",
        "        boxes2 = convert_coordinates(boxes2, start_index=0, conversion='centroids2corners')\n",
        "        coords = 'corners'\n",
        "    elif not (coords in {'minmax', 'corners'}):\n",
        "        raise ValueError(\"Unexpected value for `coords`. Supported values are 'minmax', 'corners' and 'centroids'.\")\n",
        "\n",
        "    # Compute the IoU.\n",
        "\n",
        "    # Compute the interesection areas.\n",
        "\n",
        "    intersection_areas = intersection_area_(boxes1, boxes2, coords=coords, mode=mode)\n",
        "\n",
        "    m = boxes1.shape[0] # The number of boxes in `boxes1`\n",
        "    n = boxes2.shape[0] # The number of boxes in `boxes2`\n",
        "\n",
        "    # Compute the union areas.\n",
        "\n",
        "    # Set the correct coordinate indices for the respective formats.\n",
        "    if coords == 'corners':\n",
        "        xmin = 0\n",
        "        ymin = 1\n",
        "        xmax = 2\n",
        "        ymax = 3\n",
        "    elif coords == 'minmax':\n",
        "        xmin = 0\n",
        "        xmax = 1\n",
        "        ymin = 2\n",
        "        ymax = 3\n",
        "\n",
        "    if border_pixels == 'half':\n",
        "        d = 0\n",
        "    elif border_pixels == 'include':\n",
        "        d = 1 # If border pixels are supposed to belong to the bounding boxes, we have to add one pixel to any difference `xmax - xmin` or `ymax - ymin`.\n",
        "    elif border_pixels == 'exclude':\n",
        "        d = -1 # If border pixels are not supposed to belong to the bounding boxes, we have to subtract one pixel from any difference `xmax - xmin` or `ymax - ymin`.\n",
        "\n",
        "    if mode == 'outer_product':\n",
        "\n",
        "        boxes1_areas = np.tile(np.expand_dims((boxes1[:,xmax] - boxes1[:,xmin] + d) * (boxes1[:,ymax] - boxes1[:,ymin] + d), axis=1), reps=(1,n))\n",
        "        boxes2_areas = np.tile(np.expand_dims((boxes2[:,xmax] - boxes2[:,xmin] + d) * (boxes2[:,ymax] - boxes2[:,ymin] + d), axis=0), reps=(m,1))\n",
        "\n",
        "    elif mode == 'element-wise':\n",
        "\n",
        "        boxes1_areas = (boxes1[:,xmax] - boxes1[:,xmin] + d) * (boxes1[:,ymax] - boxes1[:,ymin] + d)\n",
        "        boxes2_areas = (boxes2[:,xmax] - boxes2[:,xmin] + d) * (boxes2[:,ymax] - boxes2[:,ymin] + d)\n",
        "\n",
        "    union_areas = boxes1_areas + boxes2_areas - intersection_areas\n",
        "\n",
        "    return intersection_areas / union_areas\n"
      ],
      "metadata": {
        "id": "0-dWKQpO-I2i"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "A custom Keras layer to generate anchor boxes.\n",
        "\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.layers import InputSpec\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "# from keras.engine.topology import Layer\n",
        "\n",
        "\n",
        "class AnchorBoxes(Layer):\n",
        "    '''\n",
        "    A Keras layer to create an output tensor containing anchor box coordinates\n",
        "    and variances based on the input tensor and the passed arguments.\n",
        "\n",
        "    A set of 2D anchor boxes of different aspect ratios is created for each spatial unit of\n",
        "    the input tensor. The number of anchor boxes created per unit depends on the arguments\n",
        "    `aspect_ratios` and `two_boxes_for_ar1`, in the default case it is 4. The boxes\n",
        "    are parameterized by the coordinate tuple `(xmin, xmax, ymin, ymax)`.\n",
        "\n",
        "    The logic implemented by this layer is identical to the logic in the module\n",
        "    `ssd_box_encode_decode_utils.py`.\n",
        "\n",
        "    The purpose of having this layer in the network is to make the model self-sufficient\n",
        "    at inference time. Since the model is predicting offsets to the anchor boxes\n",
        "    (rather than predicting absolute box coordinates directly), one needs to know the anchor\n",
        "    box coordinates in order to construct the final prediction boxes from the predicted offsets.\n",
        "    If the model's output tensor did not contain the anchor box coordinates, the necessary\n",
        "    information to convert the predicted offsets back to absolute coordinates would be missing\n",
        "    in the model output. The reason why it is necessary to predict offsets to the anchor boxes\n",
        "    rather than to predict absolute box coordinates directly is explained in `README.md`.\n",
        "\n",
        "    Input shape:\n",
        "        4D tensor of shape `(batch, channels, height, width)` if `dim_ordering = 'th'`\n",
        "        or `(batch, height, width, channels)` if `dim_ordering = 'tf'`.\n",
        "\n",
        "    Output shape:\n",
        "        5D tensor of shape `(batch, height, width, n_boxes, 8)`. The last axis contains\n",
        "        the four anchor box coordinates and the four variance values for each box.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 img_height,\n",
        "                 img_width,\n",
        "                 this_scale,\n",
        "                 next_scale,\n",
        "                 aspect_ratios=[0.5, 1.0, 2.0],\n",
        "                 two_boxes_for_ar1=True,\n",
        "                 this_steps=None,\n",
        "                 this_offsets=None,\n",
        "                 clip_boxes=False,\n",
        "                 variances=[0.1, 0.1, 0.2, 0.2],\n",
        "                 coords='centroids',\n",
        "                 normalize_coords=False,\n",
        "                 **kwargs):\n",
        "        '''\n",
        "        All arguments need to be set to the same values as in the box encoding process, otherwise the behavior is undefined.\n",
        "        Some of these arguments are explained in more detail in the documentation of the `SSDBoxEncoder` class.\n",
        "\n",
        "        Arguments:\n",
        "            img_height (int): The height of the input images.\n",
        "            img_width (int): The width of the input images.\n",
        "            this_scale (float): A float in [0, 1], the scaling factor for the size of the generated anchor boxes\n",
        "                as a fraction of the shorter side of the input image.\n",
        "            next_scale (float): A float in [0, 1], the next larger scaling factor. Only relevant if\n",
        "                `self.two_boxes_for_ar1 == True`.\n",
        "            aspect_ratios (list, optional): The list of aspect ratios for which default boxes are to be\n",
        "                generated for this layer.\n",
        "            two_boxes_for_ar1 (bool, optional): Only relevant if `aspect_ratios` contains 1.\n",
        "                If `True`, two default boxes will be generated for aspect ratio 1. The first will be generated\n",
        "                using the scaling factor for the respective layer, the second one will be generated using\n",
        "                geometric mean of said scaling factor and next bigger scaling factor.\n",
        "            clip_boxes (bool, optional): If `True`, clips the anchor box coordinates to stay within image boundaries.\n",
        "            variances (list, optional): A list of 8 floats >0. The anchor box offset for each coordinate will be divided by\n",
        "                its respective variance value.\n",
        "            coords (str, optional): The box coordinate format to be used internally in the model (i.e. this is not the input format\n",
        "                of the ground truth labels). Can be either 'centroids' for the format `(cx, cy, w, h)` (box center coordinates, width, and height),\n",
        "                'corners' for the format `(xmin, ymin, xmax,  ymax)`, or 'minmax' for the format `(xmin, xmax, ymin, ymax)`.\n",
        "            normalize_coords (bool, optional): Set to `True` if the model uses relative instead of absolute coordinates,\n",
        "                i.e. if the model predicts box coordinates within [0,1] instead of absolute coordinates.\n",
        "        '''\n",
        "        if K.backend() != 'tensorflow':\n",
        "            raise TypeError(\"This layer only supports TensorFlow at the moment, but you are using the {} backend.\".format(K.backend()))\n",
        "\n",
        "        if (this_scale < 0) or (next_scale < 0) or (this_scale > 1):\n",
        "            raise ValueError(\"`this_scale` must be in [0, 1] and `next_scale` must be >0, but `this_scale` == {}, `next_scale` == {}\".format(this_scale, next_scale))\n",
        "\n",
        "        if len(variances) != 8:\n",
        "            raise ValueError(\"8 variance values must be pased, but {} values were received.\".format(len(variances)))\n",
        "        variances = np.array(variances)\n",
        "        if np.any(variances <= 0):\n",
        "            raise ValueError(\"All variances must be >0, but the variances given are {}\".format(variances))\n",
        "\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.this_scale = this_scale\n",
        "        self.next_scale = next_scale\n",
        "        self.aspect_ratios = aspect_ratios\n",
        "        self.two_boxes_for_ar1 = two_boxes_for_ar1\n",
        "        self.this_steps = this_steps\n",
        "        self.this_offsets = this_offsets\n",
        "        self.clip_boxes = clip_boxes\n",
        "        self.variances = variances\n",
        "        self.coords = coords\n",
        "        self.normalize_coords = normalize_coords\n",
        "        # Compute the number of boxes per cell\n",
        "        if (1 in aspect_ratios) and two_boxes_for_ar1:\n",
        "            self.n_boxes = len(aspect_ratios) + 1\n",
        "        else:\n",
        "            self.n_boxes = len(aspect_ratios)\n",
        "        super(AnchorBoxes, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        super(AnchorBoxes, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        '''\n",
        "        Return an anchor box tensor based on the shape of the input tensor.\n",
        "\n",
        "        The logic implemented here is identical to the logic in the module `ssd_box_encode_decode_utils.py`.\n",
        "\n",
        "        Note that this tensor does not participate in any graph computations at runtime. It is being created\n",
        "        as a constant once during graph creation and is just being output along with the rest of the model output\n",
        "        during runtime. Because of this, all logic is implemented as Numpy array operations and it is sufficient\n",
        "        to convert the resulting Numpy array into a Keras tensor at the very end before outputting it.\n",
        "\n",
        "        Arguments:\n",
        "            x (tensor): 8D tensor of shape `(batch, channels, height, width)` if `dim_ordering = 'th'`\n",
        "                or `(batch, height, width, channels)` if `dim_ordering = 'tf'`. The input for this\n",
        "                layer must be the output of the localization predictor layer.\n",
        "        '''\n",
        "\n",
        "        # Compute box width and height for each aspect ratio\n",
        "        # The shorter side of the image will be used to compute `w` and `h` using `scale` and `aspect_ratios`.\n",
        "        size = min(self.img_height, self.img_width)\n",
        "        # Compute the box widths and and heights for all aspect ratios\n",
        "        wh_list = []\n",
        "        for ar in self.aspect_ratios:\n",
        "            if (ar == 1):\n",
        "                # Compute the regular anchor box for aspect ratio 1.\n",
        "                box_height = box_width = self.this_scale * size\n",
        "                wh_list.append((box_width, box_height))\n",
        "                if self.two_boxes_for_ar1:\n",
        "                    # Compute one slightly larger version using the geometric mean of this scale value and the next.\n",
        "                    box_height = box_width = np.sqrt(self.this_scale * self.next_scale) * size\n",
        "                    wh_list.append((box_width, box_height))\n",
        "            else:\n",
        "                box_height = self.this_scale * size / np.sqrt(ar)\n",
        "                box_width = self.this_scale * size * np.sqrt(ar)\n",
        "                wh_list.append((box_width, box_height))\n",
        "        wh_list = np.array(wh_list)\n",
        "\n",
        "        # We need the shape of the input tensor\n",
        "        #if K.image_data_format() == 'tf':\n",
        "        #    batch_size, feature_map_height, feature_map_width, feature_map_channels = x.shape\n",
        "        #else: # Not yet relevant since TensorFlow is the only supported backend right now, but it can't harm to have this in here for the future\n",
        "        #    batch_size, feature_map_channels, feature_map_height, feature_map_width = x.shape\n",
        "\n",
        "        batch_size, feature_map_height, feature_map_width, feature_map_channels = x.shape\n",
        "        # Compute the grid of box center points. They are identical for all aspect ratios.\n",
        "\n",
        "        # Compute the step sizes, i.e. how far apart the anchor box center points will be vertically and horizontally.\n",
        "        if (self.this_steps is None):\n",
        "            step_height = self.img_height / feature_map_height\n",
        "            step_width = self.img_width / feature_map_width\n",
        "        else:\n",
        "            if isinstance(self.this_steps, (list, tuple)) and (len(self.this_steps) == 2):\n",
        "                step_height = self.this_steps[0]\n",
        "                step_width = self.this_steps[1]\n",
        "            elif isinstance(self.this_steps, (int, float)):\n",
        "                step_height = self.this_steps\n",
        "                step_width = self.this_steps\n",
        "        # Compute the offsets, i.e. at what pixel values the first anchor box center point will be from the top and from the left of the image.\n",
        "        if (self.this_offsets is None):\n",
        "            offset_height = 0.5\n",
        "            offset_width = 0.5\n",
        "        else:\n",
        "            if isinstance(self.this_offsets, (list, tuple)) and (len(self.this_offsets) == 2):\n",
        "                offset_height = self.this_offsets[0]\n",
        "                offset_width = self.this_offsets[1]\n",
        "            elif isinstance(self.this_offsets, (int, float)):\n",
        "                offset_height = self.this_offsets\n",
        "                offset_width = self.this_offsets\n",
        "        # Now that we have the offsets and step sizes, compute the grid of anchor box center points.\n",
        "        cy = np.linspace(offset_height * step_height, (offset_height + feature_map_height - 1) * step_height, feature_map_height)\n",
        "        cx = np.linspace(offset_width * step_width, (offset_width + feature_map_width - 1) * step_width, feature_map_width)\n",
        "        \n",
        "        cx_grid, cy_grid = np.meshgrid(cx, cy)\n",
        "        \n",
        "        cx_grid = np.expand_dims(cx_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
        "        cy_grid = np.expand_dims(cy_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
        "\n",
        "        # Create a 8D tensor template of shape `(feature_map_height, feature_map_width, n_boxes, 8)`\n",
        "        # where the last dimension will contain `(cx, cy, w, h)`\n",
        "        boxes_tensor = np.zeros((feature_map_height, feature_map_width, self.n_boxes, 8))\n",
        "\n",
        "        boxes_tensor[:, :, :, 0] = np.tile(cx_grid, (1, 1, self.n_boxes)) # Set cx\n",
        "        boxes_tensor[:, :, :, 1] = np.tile(cy_grid, (1, 1, self.n_boxes)) # Set cy\n",
        "        boxes_tensor[:, :, :, 2] = np.tile(cx_grid, (1, 1, self.n_boxes)) # Set cx\n",
        "        boxes_tensor[:, :, :, 3] = np.tile(cy_grid, (1, 1, self.n_boxes)) # Set cy\n",
        "        boxes_tensor[:, :, :, 4] = np.tile(cx_grid, (1, 1, self.n_boxes)) # Set cx\n",
        "        boxes_tensor[:, :, :, 5] = np.tile(cy_grid, (1, 1, self.n_boxes)) # Set cy\n",
        "        boxes_tensor[:, :, :, 6] = np.tile(cx_grid, (1, 1, self.n_boxes)) # Set cx\n",
        "        boxes_tensor[:, :, :, 7] = np.tile(cy_grid, (1, 1, self.n_boxes)) # Set cy\n",
        "\n",
        "        # Convert `(cx, cy, w, h)` to `(xmin, xmax, ymin, ymax)`\n",
        "        boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='centroids2corners')\n",
        "\n",
        "        # If `clip_boxes` is enabled, clip the coordinates to lie within the image boundaries\n",
        "        if self.clip_boxes:\n",
        "            x_coords = boxes_tensor[:,:,:,[0, 2, 4, 6]]\n",
        "            x_coords[x_coords >= self.img_width] = self.img_width - 1\n",
        "            x_coords[x_coords < 0] = 0\n",
        "            boxes_tensor[:,:,:,[0, 2, 4, 6]] = x_coords\n",
        "            y_coords = boxes_tensor[:,:,:,[1, 3, 5, 7]]\n",
        "            y_coords[y_coords >= self.img_height] = self.img_height - 1\n",
        "            y_coords[y_coords < 0] = 0\n",
        "            boxes_tensor[:,:,:,[1, 3, 5, 7]] = y_coords\n",
        "\n",
        "        # If `normalize_coords` is enabled, normalize the coordinates to be within [0,1]\n",
        "        if self.normalize_coords:\n",
        "            boxes_tensor[:, :, :, [0, 2, 4, 6]] /= self.img_width\n",
        "            boxes_tensor[:, :, :, [1, 3, 5, 7]] /= self.img_height\n",
        "\n",
        "        # TODO: Implement box limiting directly for `(cx, cy, w, h)` so that we don't have to unnecessarily convert back and forth.\n",
        "        #f self.coords == 'centroids':\n",
        "            # Convert `(xmin, ymin, xmax, ymax)` back to `(cx, cy, w, h)`.\n",
        "            #boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2centroids', border_pixels='half')\n",
        "        #elif self.coords == 'minmax':\n",
        "            # Convert `(xmin, ymin, xmax, ymax)` to `(xmin, xmax, ymin, ymax).\n",
        "            #boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2minmax', border_pixels='half')\n",
        "\n",
        "        # Create a tensor to contain the variances and append it to `boxes_tensor`. This tensor has the same shape\n",
        "        # as `boxes_tensor` and simply contains the same 4 variance values for every position in the last axis.\n",
        "        variances_tensor = np.zeros_like(boxes_tensor) # Has shape `(feature_map_height, feature_map_width, n_boxes, 4)`\n",
        "        variances_tensor += self.variances # Long live broadcasting\n",
        "        # Now `boxes_tensor` becomes a tensor of shape `(feature_map_height, feature_map_width, n_boxes, 8)`\n",
        "        boxes_tensor = np.concatenate((boxes_tensor, variances_tensor), axis=-1)\n",
        "\n",
        "        # Now prepend one dimension to `boxes_tensor` to account for the batch size and tile it along\n",
        "        # The result will be a 5D tensor of shape `(batch_size, feature_map_height, feature_map_width, n_boxes, 8)`\n",
        "        boxes_tensor = np.expand_dims(boxes_tensor, axis=0)\n",
        "        boxes_tensor = K.tile(K.constant(boxes_tensor, dtype='float32'), (K.shape(x)[0], 1, 1, 1, 1))\n",
        "\n",
        "        return boxes_tensor\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        #if K.image_dim_ordering() == 'tf':\n",
        "            #batch_size, feature_map_height, feature_map_width, feature_map_channels = input_shape\n",
        "        #else: # Not yet relevant since TensorFlow is the only supported backend right now, but it can't harm to have this in here for the future\n",
        "            #batch_size, feature_map_channels, feature_map_height, feature_map_width = input_shape\n",
        "        \n",
        "        return (batch_size, feature_map_height, feature_map_width, self.n_boxes, 8)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'img_height': self.img_height,\n",
        "            'img_width': self.img_width,\n",
        "            'this_scale': self.this_scale,\n",
        "            'next_scale': self.next_scale,\n",
        "            'aspect_ratios': list(self.aspect_ratios),\n",
        "            'two_boxes_for_ar1': self.two_boxes_for_ar1,\n",
        "            'clip_boxes': self.clip_boxes,\n",
        "            'variances': list(self.variances),\n",
        "            'coords': self.coords,\n",
        "            'normalize_coords': self.normalize_coords\n",
        "        }\n",
        "        base_config = super(AnchorBoxes, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "metadata": {
        "id": "EhxeIozOmqeO"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "A custom Keras layer to decode the raw SSD prediction output. Corresponds to the\n",
        "`DetectionOutput` layer type in the original Caffe implementation of SSD.\n",
        "\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "\n",
        "class DecodeDetections(Layer):\n",
        "    '''\n",
        "    A Keras layer to decode the raw SSD prediction output.\n",
        "\n",
        "    Input shape:\n",
        "        3D tensor of shape `(batch_size, n_boxes, n_classes + 12)`.\n",
        "\n",
        "    Output shape:\n",
        "        3D tensor of shape `(batch_size, top_k, 6)`.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 confidence_thresh=0.01,\n",
        "                 iou_threshold=0.45,\n",
        "                 top_k=200,\n",
        "                 nms_max_output_size=400,\n",
        "                 coords='centroids',\n",
        "                 normalize_coords=True,\n",
        "                 img_height=None,\n",
        "                 img_width=None,\n",
        "                 **kwargs):\n",
        "        '''\n",
        "        All default argument values follow the Caffe implementation.\n",
        "\n",
        "        Arguments:\n",
        "            confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in a specific\n",
        "                positive class in order to be considered for the non-maximum suppression stage for the respective class.\n",
        "                A lower value will result in a larger part of the selection process being done by the non-maximum suppression\n",
        "                stage, while a larger value will result in a larger part of the selection process happening in the confidence\n",
        "                thresholding stage.\n",
        "            iou_threshold (float, optional): A float in [0,1]. All boxes with a Jaccard similarity of greater than `iou_threshold`\n",
        "                with a locally maximal box will be removed from the set of predictions for a given class, where 'maximal' refers\n",
        "                to the box score.\n",
        "            top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n",
        "                non-maximum suppression stage.\n",
        "            nms_max_output_size (int, optional): The maximum number of predictions that will be left after performing non-maximum\n",
        "                suppression.\n",
        "            coords (str, optional): The box coordinate format that the model outputs. Must be 'centroids'\n",
        "                i.e. the format `(cx, cy, w, h)` (box center coordinates, width, and height). Other coordinate formats are\n",
        "                currently not supported.\n",
        "            normalize_coords (bool, optional): Set to `True` if the model outputs relative coordinates (i.e. coordinates in [0,1])\n",
        "                and you wish to transform these relative coordinates back to absolute coordinates. If the model outputs\n",
        "                relative coordinates, but you do not want to convert them back to absolute coordinates, set this to `False`.\n",
        "                Do not set this to `True` if the model already outputs absolute coordinates, as that would result in incorrect\n",
        "                coordinates. Requires `img_height` and `img_width` if set to `True`.\n",
        "            img_height (int, optional): The height of the input images. Only needed if `normalize_coords` is `True`.\n",
        "            img_width (int, optional): The width of the input images. Only needed if `normalize_coords` is `True`.\n",
        "        '''\n",
        "        if K.backend() != 'tensorflow':\n",
        "            raise TypeError(\"This layer only supports TensorFlow at the moment, but you are using the {} backend.\".format(K.backend()))\n",
        "\n",
        "        if normalize_coords and ((img_height is None) or (img_width is None)):\n",
        "            raise ValueError(\"If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`\".format(img_height, img_width))\n",
        "\n",
        "        if coords != 'centroids':\n",
        "            raise ValueError(\"The DetectionOutput layer currently only supports the 'centroids' coordinate format.\")\n",
        "\n",
        "        # We need these members for the config.\n",
        "        self.confidence_thresh = confidence_thresh\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.top_k = top_k\n",
        "        self.normalize_coords = normalize_coords\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.coords = coords\n",
        "        self.nms_max_output_size = nms_max_output_size\n",
        "\n",
        "        # We need these members for TensorFlow.\n",
        "        self.tf_confidence_thresh = tf.constant(self.confidence_thresh, name='confidence_thresh')\n",
        "        self.tf_iou_threshold = tf.constant(self.iou_threshold, name='iou_threshold')\n",
        "        self.tf_top_k = tf.constant(self.top_k, name='top_k')\n",
        "        self.tf_normalize_coords = tf.constant(self.normalize_coords, name='normalize_coords')\n",
        "        self.tf_img_height = tf.constant(self.img_height, dtype=tf.float32, name='img_height')\n",
        "        self.tf_img_width = tf.constant(self.img_width, dtype=tf.float32, name='img_width')\n",
        "        self.tf_nms_max_output_size = tf.constant(self.nms_max_output_size, name='nms_max_output_size')\n",
        "\n",
        "        super(DecodeDetections, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        super(DecodeDetections, self).build(input_shape)\n",
        "\n",
        "    def call(self, y_pred, mask=None):\n",
        "        '''\n",
        "        Returns:\n",
        "            3D tensor of shape `(batch_size, top_k, 6)`. The second axis is zero-padded\n",
        "            to always yield `top_k` predictions per batch item. The last axis contains\n",
        "            the coordinates for each predicted box in the format\n",
        "            `[class_id, confidence, xmin, ymin, xmax, ymax]`.\n",
        "        '''\n",
        "\n",
        "        #####################################################################################\n",
        "        # 1. Convert the box coordinates from predicted anchor box offsets to predicted\n",
        "        #    absolute coordinates\n",
        "        #####################################################################################\n",
        "\n",
        "        # Convert anchor box offsets to image offsets.\n",
        "        cx = y_pred[...,-12] * y_pred[...,-4] * y_pred[...,-6] + y_pred[...,-8] # cx = cx_pred * cx_variance * w_anchor + cx_anchor\n",
        "        cy = y_pred[...,-11] * y_pred[...,-3] * y_pred[...,-5] + y_pred[...,-7] # cy = cy_pred * cy_variance * h_anchor + cy_anchor\n",
        "        w = tf.exp(y_pred[...,-10] * y_pred[...,-2]) * y_pred[...,-6] # w = exp(w_pred * variance_w) * w_anchor\n",
        "        h = tf.exp(y_pred[...,-9] * y_pred[...,-1]) * y_pred[...,-5] # h = exp(h_pred * variance_h) * h_anchor\n",
        "\n",
        "        # Convert 'centroids' to 'corners'.\n",
        "        xmin = cx - 0.5 * w\n",
        "        ymin = cy - 0.5 * h\n",
        "        xmax = cx + 0.5 * w\n",
        "        ymax = cy + 0.5 * h\n",
        "\n",
        "        # If the model predicts box coordinates relative to the image dimensions and they are supposed\n",
        "        # to be converted back to absolute coordinates, do that.\n",
        "        def normalized_coords():\n",
        "            xmin1 = tf.expand_dims(xmin * self.tf_img_width, axis=-1)\n",
        "            ymin1 = tf.expand_dims(ymin * self.tf_img_height, axis=-1)\n",
        "            xmax1 = tf.expand_dims(xmax * self.tf_img_width, axis=-1)\n",
        "            ymax1 = tf.expand_dims(ymax * self.tf_img_height, axis=-1)\n",
        "            return xmin1, ymin1, xmax1, ymax1\n",
        "        def non_normalized_coords():\n",
        "            return tf.expand_dims(xmin, axis=-1), tf.expand_dims(ymin, axis=-1), tf.expand_dims(xmax, axis=-1), tf.expand_dims(ymax, axis=-1)\n",
        "\n",
        "        xmin, ymin, xmax, ymax = tf.cond(self.tf_normalize_coords, normalized_coords, non_normalized_coords)\n",
        "\n",
        "        # Concatenate the one-hot class confidences and the converted box coordinates to form the decoded predictions tensor.\n",
        "        y_pred = tf.concat(values=[y_pred[...,:-12], xmin, ymin, xmax, ymax], axis=-1)\n",
        "\n",
        "        #####################################################################################\n",
        "        # 2. Perform confidence thresholding, per-class non-maximum suppression, and\n",
        "        #    top-k filtering.\n",
        "        #####################################################################################\n",
        "\n",
        "        batch_size = tf.shape(y_pred)[0] # Output dtype: tf.int32\n",
        "        n_boxes = tf.shape(y_pred)[1]\n",
        "        n_classes = y_pred.shape[2] - 4\n",
        "        class_indices = tf.range(1, n_classes)\n",
        "\n",
        "        # Create a function that filters the predictions for the given batch item. Specifically, it performs:\n",
        "        # - confidence thresholding\n",
        "        # - non-maximum suppression (NMS)\n",
        "        # - top-k filtering\n",
        "        def filter_predictions(batch_item):\n",
        "\n",
        "            # Create a function that filters the predictions for one single class.\n",
        "            def filter_single_class(index):\n",
        "\n",
        "                # From a tensor of shape (n_boxes, n_classes + 4 coordinates) extract\n",
        "                # a tensor of shape (n_boxes, 1 + 4 coordinates) that contains the\n",
        "                # confidnece values for just one class, determined by `index`.\n",
        "                confidences = tf.expand_dims(batch_item[..., index], axis=-1)\n",
        "                class_id = tf.fill(dims=tf.shape(confidences), value=tf.cast(index, dtype=tf.float32))\n",
        "                box_coordinates = batch_item[...,-4:]\n",
        "\n",
        "                single_class = tf.concat([class_id, confidences, box_coordinates], axis=-1)\n",
        "\n",
        "                # Apply confidence thresholding with respect to the class defined by `index`.\n",
        "                threshold_met = single_class[:,1] > self.tf_confidence_thresh\n",
        "                single_class = tf.boolean_mask(tensor=single_class,\n",
        "                                               mask=threshold_met)\n",
        "\n",
        "                # If any boxes made the threshold, perform NMS.\n",
        "                def perform_nms():\n",
        "                    scores = single_class[...,1]\n",
        "\n",
        "                    # `tf.image.non_max_suppression()` needs the box coordinates in the format `(ymin, xmin, ymax, xmax)`.\n",
        "                    xmin = tf.expand_dims(single_class[...,-4], axis=-1)\n",
        "                    ymin = tf.expand_dims(single_class[...,-3], axis=-1)\n",
        "                    xmax = tf.expand_dims(single_class[...,-2], axis=-1)\n",
        "                    ymax = tf.expand_dims(single_class[...,-1], axis=-1)\n",
        "                    boxes = tf.concat(values=[ymin, xmin, ymax, xmax], axis=-1)\n",
        "\n",
        "                    maxima_indices = tf.image.non_max_suppression(boxes=boxes,\n",
        "                                                                  scores=scores,\n",
        "                                                                  max_output_size=self.tf_nms_max_output_size,\n",
        "                                                                  iou_threshold=self.iou_threshold,\n",
        "                                                                  name='non_maximum_suppresion')\n",
        "                    maxima = tf.gather(params=single_class,\n",
        "                                       indices=maxima_indices,\n",
        "                                       axis=0)\n",
        "                    return maxima\n",
        "\n",
        "                def no_confident_predictions():\n",
        "                    return tf.constant(value=0.0, shape=(1,6))\n",
        "\n",
        "                single_class_nms = tf.cond(tf.equal(tf.size(single_class), 0), no_confident_predictions, perform_nms)\n",
        "\n",
        "                # Make sure `single_class` is exactly `self.nms_max_output_size` elements long.\n",
        "                padded_single_class = tf.pad(tensor=single_class_nms,\n",
        "                                             paddings=[[0, self.tf_nms_max_output_size - tf.shape(single_class_nms)[0]], [0, 0]],\n",
        "                                             mode='CONSTANT',\n",
        "                                             constant_values=0.0)\n",
        "\n",
        "                return padded_single_class\n",
        "\n",
        "            # Iterate `filter_single_class()` over all class indices.\n",
        "            filtered_single_classes = tf.map_fn(fn=lambda i: filter_single_class(i),\n",
        "                                                elems=tf.range(1,n_classes),\n",
        "                                                dtype=tf.float32,\n",
        "                                                parallel_iterations=128,\n",
        "                                                back_prop=False,\n",
        "                                                swap_memory=False,\n",
        "                                                infer_shape=True,\n",
        "                                                name='loop_over_classes')\n",
        "\n",
        "            # Concatenate the filtered results for all individual classes to one tensor.\n",
        "            filtered_predictions = tf.reshape(tensor=filtered_single_classes, shape=(-1,6))\n",
        "\n",
        "            # Perform top-k filtering for this batch item or pad it in case there are\n",
        "            # fewer than `self.top_k` boxes left at this point. Either way, produce a\n",
        "            # tensor of length `self.top_k`. By the time we return the final results tensor\n",
        "            # for the whole batch, all batch items must have the same number of predicted\n",
        "            # boxes so that the tensor dimensions are homogenous. If fewer than `self.top_k`\n",
        "            # predictions are left after the filtering process above, we pad the missing\n",
        "            # predictions with zeros as dummy entries.\n",
        "            def top_k():\n",
        "                return tf.gather(params=filtered_predictions,\n",
        "                                 indices=tf.nn.top_k(filtered_predictions[:, 1], k=self.tf_top_k, sorted=True).indices,\n",
        "                                 axis=0)\n",
        "            def pad_and_top_k():\n",
        "                padded_predictions = tf.pad(tensor=filtered_predictions,\n",
        "                                            paddings=[[0, self.tf_top_k - tf.shape(filtered_predictions)[0]], [0, 0]],\n",
        "                                            mode='CONSTANT',\n",
        "                                            constant_values=0.0)\n",
        "                return tf.gather(params=padded_predictions,\n",
        "                                 indices=tf.nn.top_k(padded_predictions[:, 1], k=self.tf_top_k, sorted=True).indices,\n",
        "                                 axis=0)\n",
        "\n",
        "            top_k_boxes = tf.cond(tf.greater_equal(tf.shape(filtered_predictions)[0], self.tf_top_k), top_k, pad_and_top_k)\n",
        "\n",
        "            return top_k_boxes\n",
        "\n",
        "        # Iterate `filter_predictions()` over all batch items.\n",
        "        output_tensor = tf.map_fn(fn=lambda x: filter_predictions(x),\n",
        "                                  elems=y_pred,\n",
        "                                  dtype=None,\n",
        "                                  parallel_iterations=128,\n",
        "                                  back_prop=False,\n",
        "                                  swap_memory=False,\n",
        "                                  infer_shape=True,\n",
        "                                  name='loop_over_batch')\n",
        "\n",
        "        return output_tensor\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        batch_size, n_boxes, last_axis = input_shape\n",
        "        return (batch_size, self.tf_top_k, 6) # Last axis: (class_ID, confidence, 4 box coordinates)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'confidence_thresh': self.confidence_thresh,\n",
        "            'iou_threshold': self.iou_threshold,\n",
        "            'top_k': self.top_k,\n",
        "            'nms_max_output_size': self.nms_max_output_size,\n",
        "            'coords': self.coords,\n",
        "            'normalize_coords': self.normalize_coords,\n",
        "            'img_height': self.img_height,\n",
        "            'img_width': self.img_width,\n",
        "        }\n",
        "        base_config = super(DecodeDetections, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "metadata": {
        "id": "M9KoNqf-mqbd"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "A custom Keras layer to decode the raw SSD prediction output. This is a modified\n",
        "and more efficient version of the `DetectionOutput` layer type in the original Caffe\n",
        "implementation of SSD. For a faithful replication of the original layer, please\n",
        "refer to the `DecodeDetections` layer.\n",
        "\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.layers import InputSpec\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "\n",
        "class DecodeDetectionsFast(Layer):\n",
        "    '''\n",
        "    A Keras layer to decode the raw SSD prediction output.\n",
        "\n",
        "    Input shape:\n",
        "        3D tensor of shape `(batch_size, n_boxes, n_classes + 12)`.\n",
        "\n",
        "    Output shape:\n",
        "        3D tensor of shape `(batch_size, top_k, 6)`.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 confidence_thresh=0.01,\n",
        "                 iou_threshold=0.45,\n",
        "                 top_k=200,\n",
        "                 nms_max_output_size=400,\n",
        "                 coords='centroids',\n",
        "                 normalize_coords=True,\n",
        "                 img_height=None,\n",
        "                 img_width=None,\n",
        "                 **kwargs):\n",
        "        '''\n",
        "        All default argument values follow the Caffe implementation.\n",
        "\n",
        "        Arguments:\n",
        "            confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in a specific\n",
        "                positive class in order to be considered for the non-maximum suppression stage for the respective class.\n",
        "                A lower value will result in a larger part of the selection process being done by the non-maximum suppression\n",
        "                stage, while a larger value will result in a larger part of the selection process happening in the confidence\n",
        "                thresholding stage.\n",
        "            iou_threshold (float, optional): A float in [0,1]. All boxes with a Jaccard similarity of greater than `iou_threshold`\n",
        "                with a locally maximal box will be removed from the set of predictions for a given class, where 'maximal' refers\n",
        "                to the box score.\n",
        "            top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n",
        "                non-maximum suppression stage.\n",
        "            nms_max_output_size (int, optional): The maximum number of predictions that will be left after performing non-maximum\n",
        "                suppression.\n",
        "            coords (str, optional): The box coordinate format that the model outputs. Must be 'centroids'\n",
        "                i.e. the format `(cx, cy, w, h)` (box center coordinates, width, and height). Other coordinate formats are\n",
        "                currently not supported.\n",
        "            normalize_coords (bool, optional): Set to `True` if the model outputs relative coordinates (i.e. coordinates in [0,1])\n",
        "                and you wish to transform these relative coordinates back to absolute coordinates. If the model outputs\n",
        "                relative coordinates, but you do not want to convert them back to absolute coordinates, set this to `False`.\n",
        "                Do not set this to `True` if the model already outputs absolute coordinates, as that would result in incorrect\n",
        "                coordinates. Requires `img_height` and `img_width` if set to `True`.\n",
        "            img_height (int, optional): The height of the input images. Only needed if `normalize_coords` is `True`.\n",
        "            img_width (int, optional): The width of the input images. Only needed if `normalize_coords` is `True`.\n",
        "        '''\n",
        "        if K.backend() != 'tensorflow':\n",
        "            raise TypeError(\"This layer only supports TensorFlow at the moment, but you are using the {} backend.\".format(K.backend()))\n",
        "\n",
        "        if normalize_coords and ((img_height is None) or (img_width is None)):\n",
        "            raise ValueError(\"If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`\".format(img_height, img_width))\n",
        "\n",
        "        if coords != 'centroids':\n",
        "            raise ValueError(\"The DetectionOutput layer currently only supports the 'centroids' coordinate format.\")\n",
        "\n",
        "        # We need these members for the config.\n",
        "        self.confidence_thresh = confidence_thresh\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.top_k = top_k\n",
        "        self.normalize_coords = normalize_coords\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.coords = coords\n",
        "        self.nms_max_output_size = nms_max_output_size\n",
        "\n",
        "        # We need these members for TensorFlow.\n",
        "        self.tf_confidence_thresh = tf.constant(self.confidence_thresh, name='confidence_thresh')\n",
        "        self.tf_iou_threshold = tf.constant(self.iou_threshold, name='iou_threshold')\n",
        "        self.tf_top_k = tf.constant(self.top_k, name='top_k')\n",
        "        self.tf_normalize_coords = tf.constant(self.normalize_coords, name='normalize_coords')\n",
        "        self.tf_img_height = tf.constant(self.img_height, dtype=tf.float32, name='img_height')\n",
        "        self.tf_img_width = tf.constant(self.img_width, dtype=tf.float32, name='img_width')\n",
        "        self.tf_nms_max_output_size = tf.constant(self.nms_max_output_size, name='nms_max_output_size')\n",
        "\n",
        "        super(DecodeDetectionsFast, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        super(DecodeDetectionsFast, self).build(input_shape)\n",
        "\n",
        "    def call(self, y_pred, mask=None):\n",
        "        '''\n",
        "        Returns:\n",
        "            3D tensor of shape `(batch_size, top_k, 6)`. The second axis is zero-padded\n",
        "            to always yield `top_k` predictions per batch item. The last axis contains\n",
        "            the coordinates for each predicted box in the format\n",
        "            `[class_id, confidence, xmin, ymin, xmax, ymax]`.\n",
        "        '''\n",
        "\n",
        "        #####################################################################################\n",
        "        # 1. Convert the box coordinates from predicted anchor box offsets to predicted\n",
        "        #    absolute coordinates\n",
        "        #####################################################################################\n",
        "\n",
        "        # Extract the predicted class IDs as the indices of the highest confidence values.\n",
        "        class_ids = tf.expand_dims(tf.cast(tf.argmax(y_pred[...,:-12], axis=-1)), axis=-1, dtype=tf.float32)\n",
        "        # Extract the confidences of the maximal classes.\n",
        "        confidences = tf.reduce_max(y_pred[...,:-12], axis=-1, keep_dims=True)\n",
        "\n",
        "        # Convert anchor box offsets to image offsets.\n",
        "        cx = y_pred[...,-12] * y_pred[...,-4] * y_pred[...,-6] + y_pred[...,-8] # cx = cx_pred * cx_variance * w_anchor + cx_anchor\n",
        "        cy = y_pred[...,-11] * y_pred[...,-3] * y_pred[...,-5] + y_pred[...,-7] # cy = cy_pred * cy_variance * h_anchor + cy_anchor\n",
        "        w = tf.exp(y_pred[...,-10] * y_pred[...,-2]) * y_pred[...,-6] # w = exp(w_pred * variance_w) * w_anchor\n",
        "        h = tf.exp(y_pred[...,-9] * y_pred[...,-1]) * y_pred[...,-5] # h = exp(h_pred * variance_h) * h_anchor\n",
        "\n",
        "        # Convert 'centroids' to 'corners'.\n",
        "        xmin = cx - 0.5 * w\n",
        "        ymin = cy - 0.5 * h\n",
        "        xmax = cx + 0.5 * w\n",
        "        ymax = cy + 0.5 * h\n",
        "\n",
        "        # If the model predicts box coordinates relative to the image dimensions and they are supposed\n",
        "        # to be converted back to absolute coordinates, do that.\n",
        "        def normalized_coords():\n",
        "            xmin1 = tf.expand_dims(xmin * self.tf_img_width, axis=-1)\n",
        "            ymin1 = tf.expand_dims(ymin * self.tf_img_height, axis=-1)\n",
        "            xmax1 = tf.expand_dims(xmax * self.tf_img_width, axis=-1)\n",
        "            ymax1 = tf.expand_dims(ymax * self.tf_img_height, axis=-1)\n",
        "            return xmin1, ymin1, xmax1, ymax1\n",
        "        def non_normalized_coords():\n",
        "            return tf.expand_dims(xmin, axis=-1), tf.expand_dims(ymin, axis=-1), tf.expand_dims(xmax, axis=-1), tf.expand_dims(ymax, axis=-1)\n",
        "\n",
        "        xmin, ymin, xmax, ymax = tf.cond(self.tf_normalize_coords, normalized_coords, non_normalized_coords)\n",
        "\n",
        "        # Concatenate the one-hot class confidences and the converted box coordinates to form the decoded predictions tensor.\n",
        "        y_pred = tf.concat(values=[class_ids, confidences, xmin, ymin, xmax, ymax], axis=-1)\n",
        "\n",
        "        #####################################################################################\n",
        "        # 2. Perform confidence thresholding, non-maximum suppression, and top-k filtering.\n",
        "        #####################################################################################\n",
        "\n",
        "        batch_size = tf.shape(y_pred)[0] # Output dtype: tf.int32\n",
        "        n_boxes = tf.shape(y_pred)[1]\n",
        "        n_classes = y_pred.shape[2] - 4\n",
        "        class_indices = tf.range(1, n_classes)\n",
        "\n",
        "        # Create a function that filters the predictions for the given batch item. Specifically, it performs:\n",
        "        # - confidence thresholding\n",
        "        # - non-maximum suppression (NMS)\n",
        "        # - top-k filtering\n",
        "        def filter_predictions(batch_item):\n",
        "\n",
        "            # Keep only the non-background boxes.\n",
        "            positive_boxes = tf.not_equal(batch_item[...,0], 0.0)\n",
        "            predictions = tf.boolean_mask(tensor=batch_item,\n",
        "                                          mask=positive_boxes)\n",
        "\n",
        "            def perform_confidence_thresholding():\n",
        "                # Apply confidence thresholding.\n",
        "                threshold_met = predictions[:,1] > self.tf_confidence_thresh\n",
        "                return tf.boolean_mask(tensor=predictions,\n",
        "                                       mask=threshold_met)\n",
        "            def no_positive_boxes():\n",
        "                return tf.constant(value=0.0, shape=(1,6))\n",
        "\n",
        "            # If there are any positive predictions, perform confidence thresholding.\n",
        "            predictions_conf_thresh = tf.cond(tf.equal(tf.size(predictions), 0), no_positive_boxes, perform_confidence_thresholding)\n",
        "\n",
        "            def perform_nms():\n",
        "                scores = predictions_conf_thresh[...,1]\n",
        "\n",
        "                # `tf.image.non_max_suppression()` needs the box coordinates in the format `(ymin, xmin, ymax, xmax)`.\n",
        "                xmin = tf.expand_dims(predictions_conf_thresh[...,-4], axis=-1)\n",
        "                ymin = tf.expand_dims(predictions_conf_thresh[...,-3], axis=-1)\n",
        "                xmax = tf.expand_dims(predictions_conf_thresh[...,-2], axis=-1)\n",
        "                ymax = tf.expand_dims(predictions_conf_thresh[...,-1], axis=-1)\n",
        "                boxes = tf.concat(values=[ymin, xmin, ymax, xmax], axis=-1)\n",
        "\n",
        "                maxima_indices = tf.image.non_max_suppression(boxes=boxes,\n",
        "                                                              scores=scores,\n",
        "                                                              max_output_size=self.tf_nms_max_output_size,\n",
        "                                                              iou_threshold=self.iou_threshold,\n",
        "                                                              name='non_maximum_suppresion')\n",
        "                maxima = tf.gather(params=predictions_conf_thresh,\n",
        "                                   indices=maxima_indices,\n",
        "                                   axis=0)\n",
        "                return maxima\n",
        "            def no_confident_predictions():\n",
        "                return tf.constant(value=0.0, shape=(1,6))\n",
        "\n",
        "            # If any boxes made the threshold, perform NMS.\n",
        "            predictions_nms = tf.cond(tf.equal(tf.size(predictions_conf_thresh), 0), no_confident_predictions, perform_nms)\n",
        "\n",
        "            # Perform top-k filtering for this batch item or pad it in case there are\n",
        "            # fewer than `self.top_k` boxes left at this point. Either way, produce a\n",
        "            # tensor of length `self.top_k`. By the time we return the final results tensor\n",
        "            # for the whole batch, all batch items must have the same number of predicted\n",
        "            # boxes so that the tensor dimensions are homogenous. If fewer than `self.top_k`\n",
        "            # predictions are left after the filtering process above, we pad the missing\n",
        "            # predictions with zeros as dummy entries.\n",
        "            def top_k():\n",
        "                return tf.gather(params=predictions_nms,\n",
        "                                 indices=tf.nn.top_k(predictions_nms[:, 1], k=self.tf_top_k, sorted=True).indices,\n",
        "                                 axis=0)\n",
        "            def pad_and_top_k():\n",
        "                padded_predictions = tf.pad(tensor=predictions_nms,\n",
        "                                            paddings=[[0, self.tf_top_k - tf.shape(predictions_nms)[0]], [0, 0]],\n",
        "                                            mode='CONSTANT',\n",
        "                                            constant_values=0.0)\n",
        "                return tf.gather(params=padded_predictions,\n",
        "                                 indices=tf.nn.top_k(padded_predictions[:, 1], k=self.tf_top_k, sorted=True).indices,\n",
        "                                 axis=0)\n",
        "\n",
        "            top_k_boxes = tf.cond(tf.greater_equal(tf.shape(predictions_nms)[0], self.tf_top_k), top_k, pad_and_top_k)\n",
        "\n",
        "            return top_k_boxes\n",
        "\n",
        "        # Iterate `filter_predictions()` over all batch items.\n",
        "        output_tensor = tf.map_fn(fn=lambda x: filter_predictions(x),\n",
        "                                  elems=y_pred,\n",
        "                                  dtype=None,\n",
        "                                  parallel_iterations=128,\n",
        "                                  back_prop=False,\n",
        "                                  swap_memory=False,\n",
        "                                  infer_shape=True,\n",
        "                                  name='loop_over_batch')\n",
        "\n",
        "        return output_tensor\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        batch_size, n_boxes, last_axis = input_shape\n",
        "        return (batch_size, self.tf_top_k, 6) # Last axis: (class_ID, confidence, 4 box coordinates)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'confidence_thresh': self.confidence_thresh,\n",
        "            'iou_threshold': self.iou_threshold,\n",
        "            'top_k': self.top_k,\n",
        "            'nms_max_output_size': self.nms_max_output_size,\n",
        "            'coords': self.coords,\n",
        "            'normalize_coords': self.normalize_coords,\n",
        "            'img_height': self.img_height,\n",
        "            'img_width': self.img_width,\n",
        "        }\n",
        "        base_config = super(DecodeDetectionsFast, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "metadata": {
        "id": "YhkZxwc3mqYb"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "A custom Keras layer to perform L2-normalization.\n",
        "\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.layers import InputSpec\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "import keras as kk\n",
        "\n",
        "class L2Normalization(Layer):\n",
        "    '''\n",
        "    Performs L2 normalization on the input tensor with a learnable scaling parameter\n",
        "    as described in the paper \"Parsenet: Looking Wider to See Better\" (see references)\n",
        "    and as used in the original SSD model.\n",
        "\n",
        "    Arguments:\n",
        "        gamma_init (int): The initial scaling parameter. Defaults to 20 following the\n",
        "            SSD paper.\n",
        "\n",
        "    Input shape:\n",
        "        4D tensor of shape `(batch, channels, height, width)` if `dim_ordering = 'th'`\n",
        "        or `(batch, height, width, channels)` if `dim_ordering = 'tf'`.\n",
        "\n",
        "    Returns:\n",
        "        The scaled tensor. Same shape as the input tensor.\n",
        "\n",
        "    References:\n",
        "        http://cs.unc.edu/~wliu/papers/parsenet.pdf\n",
        "    '''\n",
        "\n",
        "    def __init__(self, gamma_init=20, **kwargs):\n",
        "#        if K.image_data_format() == 'channels_last':\n",
        "#          print(\"tf\")\n",
        "#          self.axis = 3\n",
        "#        else:\n",
        "#          print(\"th\")\n",
        "#          self.axis = 1\n",
        "        \n",
        "        self.axis = 3\n",
        "        self.gamma_init = gamma_init\n",
        "        super(L2Normalization, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        gamma = self.gamma_init * np.ones((input_shape[self.axis],))\n",
        "        self.gamma = K.variable(gamma, name='{}_gamma'.format(self.name))\n",
        "        self._trainable_weights = [self.gamma]\n",
        "        super(L2Normalization, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output = K.l2_normalize(x, self.axis)\n",
        "        return output * self.gamma\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'gamma_init': self.gamma_init\n",
        "        }\n",
        "        base_config = super(L2Normalization, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "metadata": {
        "id": "NdDVl9CpmqV8"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "A Keras port of the original Caffe SSD300 network.\n",
        "\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Lambda, Activation, Conv2D, MaxPooling2D, ZeroPadding2D, Reshape, Concatenate\n",
        "from keras.regularizers import l2\n",
        "import keras.backend as K\n",
        "\n",
        "def ssd_300(image_size,\n",
        "            n_classes,\n",
        "            mode='training',\n",
        "            l2_regularization=0.0005,\n",
        "            min_scale=None,\n",
        "            max_scale=None,\n",
        "            scales=None,\n",
        "            aspect_ratios_global=None,\n",
        "            aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
        "                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                     [1.0, 2.0, 0.5],\n",
        "                                     [1.0, 2.0, 0.5]],\n",
        "            two_boxes_for_ar1=True,\n",
        "            steps=[8, 16, 32, 64, 100, 300],\n",
        "            offsets=None,\n",
        "            clip_boxes=False,\n",
        "            variances=[0.1, 0.1, 0.2, 0.2, 0.1, 0.1, 0.2, 0.2],\n",
        "            coords='centroids',\n",
        "            normalize_coords=True,\n",
        "            subtract_mean=[123, 117, 104],\n",
        "            divide_by_stddev=None,\n",
        "            swap_channels=[2, 1, 0],\n",
        "            confidence_thresh=0.01,\n",
        "            iou_threshold=0.45,\n",
        "            top_k=200,\n",
        "            nms_max_output_size=400,\n",
        "            return_predictor_sizes=False):\n",
        "    '''\n",
        "    Build a Keras model with SSD300 architecture, see references.\n",
        "\n",
        "    The base network is a reduced atrous VGG-16, extended by the SSD architecture,\n",
        "    as described in the paper.\n",
        "\n",
        "    Most of the arguments that this function takes are only needed for the anchor\n",
        "    box layers. In case you're training the network, the parameters passed here must\n",
        "    be the same as the ones used to set up `SSDBoxEncoder`. In case you're loading\n",
        "    trained weights, the parameters passed here must be the same as the ones used\n",
        "    to produce the trained weights.\n",
        "\n",
        "    Some of these arguments are explained in more detail in the documentation of the\n",
        "    `SSDBoxEncoder` class.\n",
        "\n",
        "    Note: Requires Keras v2.0 or later. Currently works only with the\n",
        "    TensorFlow backend (v1.0 or later).\n",
        "\n",
        "    Arguments:\n",
        "        image_size (tuple): The input image size in the format `(height, width, channels)`.\n",
        "        n_classes (int): The number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO.\n",
        "        mode (str, optional): One of 'training', 'inference' and 'inference_fast'. In 'training' mode,\n",
        "            the model outputs the raw prediction tensor, while in 'inference' and 'inference_fast' modes,\n",
        "            the raw predictions are decoded into absolute coordinates and filtered via confidence thresholding,\n",
        "            non-maximum suppression, and top-k filtering. The difference between latter two modes is that\n",
        "            'inference' follows the exact procedure of the original Caffe implementation, while\n",
        "            'inference_fast' uses a faster prediction decoding procedure.\n",
        "        l2_regularization (float, optional): The L2-regularization rate. Applies to all convolutional layers.\n",
        "            Set to zero to deactivate L2-regularization.\n",
        "        min_scale (float, optional): The smallest scaling factor for the size of the anchor boxes as a fraction\n",
        "            of the shorter side of the input images.\n",
        "        max_scale (float, optional): The largest scaling factor for the size of the anchor boxes as a fraction\n",
        "            of the shorter side of the input images. All scaling factors between the smallest and the\n",
        "            largest will be linearly interpolated. Note that the second to last of the linearly interpolated\n",
        "            scaling factors will actually be the scaling factor for the last predictor layer, while the last\n",
        "            scaling factor is used for the second box for aspect ratio 1 in the last predictor layer\n",
        "            if `two_boxes_for_ar1` is `True`.\n",
        "        scales (list, optional): A list of floats containing scaling factors per convolutional predictor layer.\n",
        "            This list must be one element longer than the number of predictor layers. The first `k` elements are the\n",
        "            scaling factors for the `k` predictor layers, while the last element is used for the second box\n",
        "            for aspect ratio 1 in the last predictor layer if `two_boxes_for_ar1` is `True`. This additional\n",
        "            last scaling factor must be passed either way, even if it is not being used. If a list is passed,\n",
        "            this argument overrides `min_scale` and `max_scale`. All scaling factors must be greater than zero.\n",
        "        aspect_ratios_global (list, optional): The list of aspect ratios for which anchor boxes are to be\n",
        "            generated. This list is valid for all prediction layers.\n",
        "        aspect_ratios_per_layer (list, optional): A list containing one aspect ratio list for each prediction layer.\n",
        "            This allows you to set the aspect ratios for each predictor layer individually, which is the case for the\n",
        "            original SSD300 implementation. If a list is passed, it overrides `aspect_ratios_global`.\n",
        "        two_boxes_for_ar1 (bool, optional): Only relevant for aspect ratio lists that contain 1. Will be ignored otherwise.\n",
        "            If `True`, two anchor boxes will be generated for aspect ratio 1. The first will be generated\n",
        "            using the scaling factor for the respective layer, the second one will be generated using\n",
        "            geometric mean of said scaling factor and next bigger scaling factor.\n",
        "        steps (list, optional): `None` or a list with as many elements as there are predictor layers. The elements can be\n",
        "            either ints/floats or tuples of two ints/floats. These numbers represent for each predictor layer how many\n",
        "            pixels apart the anchor box center points should be vertically and horizontally along the spatial grid over\n",
        "            the image. If the list contains ints/floats, then that value will be used for both spatial dimensions.\n",
        "            If the list contains tuples of two ints/floats, then they represent `(step_height, step_width)`.\n",
        "            If no steps are provided, then they will be computed such that the anchor box center points will form an\n",
        "            equidistant grid within the image dimensions.\n",
        "        offsets (list, optional): `None` or a list with as many elements as there are predictor layers. The elements can be\n",
        "            either floats or tuples of two floats. These numbers represent for each predictor layer how many\n",
        "            pixels from the top and left boarders of the image the top-most and left-most anchor box center points should be\n",
        "            as a fraction of `steps`. The last bit is important: The offsets are not absolute pixel values, but fractions\n",
        "            of the step size specified in the `steps` argument. If the list contains floats, then that value will\n",
        "            be used for both spatial dimensions. If the list contains tuples of two floats, then they represent\n",
        "            `(vertical_offset, horizontal_offset)`. If no offsets are provided, then they will default to 0.5 of the step size.\n",
        "        clip_boxes (bool, optional): If `True`, clips the anchor box coordinates to stay within image boundaries.\n",
        "        variances (list, optional): A list of 4 floats >0. The anchor box offset for each coordinate will be divided by\n",
        "            its respective variance value.\n",
        "        coords (str, optional): The box coordinate format to be used internally by the model (i.e. this is not the input format\n",
        "            of the ground truth labels). Can be either 'centroids' for the format `(cx, cy, w, h)` (box center coordinates, width,\n",
        "            and height), 'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.\n",
        "        normalize_coords (bool, optional): Set to `True` if the model is supposed to use relative instead of absolute coordinates,\n",
        "            i.e. if the model predicts box coordinates within [0,1] instead of absolute coordinates.\n",
        "        subtract_mean (array-like, optional): `None` or an array-like object of integers or floating point values\n",
        "            of any shape that is broadcast-compatible with the image shape. The elements of this array will be\n",
        "            subtracted from the image pixel intensity values. For example, pass a list of three integers\n",
        "            to perform per-channel mean normalization for color images.\n",
        "        divide_by_stddev (array-like, optional): `None` or an array-like object of non-zero integers or\n",
        "            floating point values of any shape that is broadcast-compatible with the image shape. The image pixel\n",
        "            intensity values will be divided by the elements of this array. For example, pass a list\n",
        "            of three integers to perform per-channel standard deviation normalization for color images.\n",
        "        swap_channels (list, optional): Either `False` or a list of integers representing the desired order in which the input\n",
        "            image channels should be swapped.\n",
        "        confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in a specific\n",
        "            positive class in order to be considered for the non-maximum suppression stage for the respective class.\n",
        "            A lower value will result in a larger part of the selection process being done by the non-maximum suppression\n",
        "            stage, while a larger value will result in a larger part of the selection process happening in the confidence\n",
        "            thresholding stage.\n",
        "        iou_threshold (float, optional): A float in [0,1]. All boxes that have a Jaccard similarity of greater than `iou_threshold`\n",
        "            with a locally maximal box will be removed from the set of predictions for a given class, where 'maximal' refers\n",
        "            to the box's confidence score.\n",
        "        top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n",
        "            non-maximum suppression stage.\n",
        "        nms_max_output_size (int, optional): The maximal number of predictions that will be left over after the NMS stage.\n",
        "        return_predictor_sizes (bool, optional): If `True`, this function not only returns the model, but also\n",
        "            a list containing the spatial dimensions of the predictor layers. This isn't strictly necessary since\n",
        "            you can always get their sizes easily via the Keras API, but it's convenient and less error-prone\n",
        "            to get them this way. They are only relevant for training anyway (SSDBoxEncoder needs to know the\n",
        "            spatial dimensions of the predictor layers), for inference you don't need them.\n",
        "\n",
        "    Returns:\n",
        "        model: The Keras SSD300 model.\n",
        "        predictor_sizes (optional): A Numpy array containing the `(height, width)` portion\n",
        "            of the output tensor shape for each convolutional predictor layer. During\n",
        "            training, the generator function needs this in order to transform\n",
        "            the ground truth labels into tensors of identical structure as the\n",
        "            output tensors of the model, which is in turn needed for the cost\n",
        "            function.\n",
        "\n",
        "    References:\n",
        "        https://arxiv.org/abs/1512.02325v5\n",
        "    '''\n",
        "\n",
        "    n_predictor_layers = 6 # The number of predictor conv layers in the network is 6 for the original SSD300.\n",
        "    n_classes += 1 # Account for the background class.\n",
        "    l2_reg = l2_regularization # Make the internal name shorter.\n",
        "    img_height, img_width, img_channels = image_size[0], image_size[1], image_size[2]\n",
        "\n",
        "    ############################################################################\n",
        "    # Get a few exceptions out of the way.\n",
        "    ############################################################################\n",
        "\n",
        "    if aspect_ratios_global is None and aspect_ratios_per_layer is None:\n",
        "        raise ValueError(\"`aspect_ratios_global` and `aspect_ratios_per_layer` cannot both be None. At least one needs to be specified.\")\n",
        "    if aspect_ratios_per_layer:\n",
        "        if len(aspect_ratios_per_layer) != n_predictor_layers:\n",
        "            raise ValueError(\"It must be either aspect_ratios_per_layer is None or len(aspect_ratios_per_layer) == {}, but len(aspect_ratios_per_layer) == {}.\".format(n_predictor_layers, len(aspect_ratios_per_layer)))\n",
        "\n",
        "    if (min_scale is None or max_scale is None) and scales is None:\n",
        "        raise ValueError(\"Either `min_scale` and `max_scale` or `scales` need to be specified.\")\n",
        "    if scales:\n",
        "        if len(scales) != n_predictor_layers+1:\n",
        "            raise ValueError(\"It must be either scales is None or len(scales) == {}, but len(scales) == {}.\".format(n_predictor_layers+1, len(scales)))\n",
        "    else: # If no explicit list of scaling factors was passed, compute the list of scaling factors from `min_scale` and `max_scale`\n",
        "        scales = np.linspace(min_scale, max_scale, n_predictor_layers+1)\n",
        "\n",
        "    if len(variances) != 8:\n",
        "        raise ValueError(\"8 variance values must be pased, but {} values were received.\".format(len(variances)))\n",
        "    variances = np.array(variances)\n",
        "    if np.any(variances <= 0):\n",
        "        raise ValueError(\"All variances must be >0, but the variances given are {}\".format(variances))\n",
        "\n",
        "    if (not (steps is None)) and (len(steps) != n_predictor_layers):\n",
        "        raise ValueError(\"You must provide at least one step value per predictor layer.\")\n",
        "\n",
        "    if (not (offsets is None)) and (len(offsets) != n_predictor_layers):\n",
        "        raise ValueError(\"You must provide at least one offset value per predictor layer.\")\n",
        "\n",
        "    ############################################################################\n",
        "    # Compute the anchor box parameters.\n",
        "    ############################################################################\n",
        "\n",
        "    # Set the aspect ratios for each predictor layer. These are only needed for the anchor box layers.\n",
        "    if aspect_ratios_per_layer:\n",
        "        aspect_ratios = aspect_ratios_per_layer\n",
        "    else:\n",
        "        aspect_ratios = [aspect_ratios_global] * n_predictor_layers\n",
        "\n",
        "    # Compute the number of boxes to be predicted per cell for each predictor layer.\n",
        "    # We need this so that we know how many channels the predictor layers need to have.\n",
        "    if aspect_ratios_per_layer:\n",
        "        n_boxes = []\n",
        "        for ar in aspect_ratios_per_layer:\n",
        "            if (1 in ar) & two_boxes_for_ar1:\n",
        "                n_boxes.append(len(ar) + 1) # +1 for the second box for aspect ratio 1\n",
        "            else:\n",
        "                n_boxes.append(len(ar))\n",
        "    else: # If only a global aspect ratio list was passed, then the number of boxes is the same for each predictor layer\n",
        "        if (1 in aspect_ratios_global) & two_boxes_for_ar1:\n",
        "            n_boxes = len(aspect_ratios_global) + 1\n",
        "        else:\n",
        "            n_boxes = len(aspect_ratios_global)\n",
        "        n_boxes = [n_boxes] * n_predictor_layers\n",
        "\n",
        "    if steps is None:\n",
        "        steps = [None] * n_predictor_layers\n",
        "    if offsets is None:\n",
        "        offsets = [None] * n_predictor_layers\n",
        "\n",
        "    ############################################################################\n",
        "    # Define functions for the Lambda layers below.\n",
        "    ############################################################################\n",
        "\n",
        "    def identity_layer(tensor):\n",
        "        return tensor\n",
        "\n",
        "    def input_mean_normalization(tensor):\n",
        "        return tensor - np.array(subtract_mean)\n",
        "\n",
        "    def input_stddev_normalization(tensor):\n",
        "        return tensor / np.array(divide_by_stddev)\n",
        "\n",
        "    def input_channel_swap(tensor):\n",
        "        if len(swap_channels) == 3:\n",
        "            return K.stack([tensor[...,swap_channels[0]], tensor[...,swap_channels[1]], tensor[...,swap_channels[2]]], axis=-1)\n",
        "        elif len(swap_channels) == 4:\n",
        "            return K.stack([tensor[...,swap_channels[0]], tensor[...,swap_channels[1]], tensor[...,swap_channels[2]], tensor[...,swap_channels[3]]], axis=-1)\n",
        "\n",
        "    ############################################################################\n",
        "    # Build the network.\n",
        "    ############################################################################\n",
        "\n",
        "    x = Input(shape=(img_height, img_width, img_channels))\n",
        "\n",
        "    # The following identity layer is only needed so that the subsequent lambda layers can be optional.\n",
        "    x1 = Lambda(identity_layer, output_shape=(img_height, img_width, img_channels), name='identity_layer')(x)\n",
        "    if not (subtract_mean is None):\n",
        "        x1 = Lambda(input_mean_normalization, output_shape=(img_height, img_width, img_channels), name='input_mean_normalization')(x1)\n",
        "    if not (divide_by_stddev is None):\n",
        "        x1 = Lambda(input_stddev_normalization, output_shape=(img_height, img_width, img_channels), name='input_stddev_normalization')(x1)\n",
        "    if swap_channels:\n",
        "        x1 = Lambda(input_channel_swap, output_shape=(img_height, img_width, img_channels), name='input_channel_swap')(x1)\n",
        "\n",
        "    conv1_1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv1_1')(x1)\n",
        "    conv1_2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv1_2')(conv1_1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool1')(conv1_2)\n",
        "\n",
        "    conv2_1 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv2_1')(pool1)\n",
        "    conv2_2 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv2_2')(conv2_1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool2')(conv2_2)\n",
        "\n",
        "    conv3_1 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3_1')(pool2)\n",
        "    conv3_2 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3_2')(conv3_1)\n",
        "    conv3_3 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3_3')(conv3_2)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool3')(conv3_3)\n",
        "\n",
        "    conv4_1 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_1')(pool3)\n",
        "    conv4_2 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_2')(conv4_1)\n",
        "    conv4_3 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3')(conv4_2)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool4')(conv4_3)\n",
        "\n",
        "    conv5_1 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5_1')(pool4)\n",
        "    conv5_2 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5_2')(conv5_1)\n",
        "    conv5_3 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5_3')(conv5_2)\n",
        "    pool5 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same', name='pool5')(conv5_3)\n",
        "\n",
        "    fc6 = Conv2D(1024, (3, 3), dilation_rate=(6, 6), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc6')(pool5)\n",
        "\n",
        "    fc7 = Conv2D(1024, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7')(fc6)\n",
        "\n",
        "    conv6_1 = Conv2D(256, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_1')(fc7)\n",
        "    conv6_1 = ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv6_padding')(conv6_1)\n",
        "    conv6_2 = Conv2D(512, (3, 3), strides=(2, 2), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2')(conv6_1)\n",
        "\n",
        "    conv7_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_1')(conv6_2)\n",
        "    conv7_1 = ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv7_padding')(conv7_1)\n",
        "    conv7_2 = Conv2D(256, (3, 3), strides=(2, 2), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2')(conv7_1)\n",
        "\n",
        "    conv8_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_1')(conv7_2)\n",
        "    conv8_2 = Conv2D(256, (3, 3), strides=(1, 1), padding='valid', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_2')(conv8_1)\n",
        "\n",
        "    conv9_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_1')(conv8_2)\n",
        "    conv9_2 = Conv2D(256, (3, 3), strides=(1, 1), padding='valid', activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_2')(conv9_1)\n",
        "\n",
        "    # Feed conv4_3 into the L2 normalization layer\n",
        "    conv4_3_norm = L2Normalization(gamma_init=20, name='conv4_3_norm')(conv4_3)\n",
        "\n",
        "    ### Build the convolutional predictor layers on top of the base network\n",
        "\n",
        "    # We precidt `n_classes` confidence values for each box, hence the confidence predictors have depth `n_boxes * n_classes`\n",
        "    # Output shape of the confidence layers: `(batch, height, width, n_boxes * n_classes)`\n",
        "    conv4_3_norm_mbox_conf = Conv2D(n_boxes[0] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3_norm_mbox_conf')(conv4_3_norm)\n",
        "    fc7_mbox_conf = Conv2D(n_boxes[1] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7_mbox_conf')(fc7)\n",
        "    conv6_2_mbox_conf = Conv2D(n_boxes[2] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2_mbox_conf')(conv6_2)\n",
        "    conv7_2_mbox_conf = Conv2D(n_boxes[3] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2_mbox_conf')(conv7_2)\n",
        "    conv8_2_mbox_conf = Conv2D(n_boxes[4] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_2_mbox_conf')(conv8_2)\n",
        "    conv9_2_mbox_conf = Conv2D(n_boxes[5] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_2_mbox_conf')(conv9_2)\n",
        "    # We predict 4 box coordinates for each box, hence the localization predictors have depth `n_boxes * 4`\n",
        "    # Output shape of the localization layers: `(batch, height, width, n_boxes * 4)`\n",
        "    conv4_3_norm_mbox_loc = Conv2D(n_boxes[0] * 8, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3_norm_mbox_loc')(conv4_3_norm)\n",
        "    fc7_mbox_loc = Conv2D(n_boxes[1] * 8, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7_mbox_loc')(fc7)\n",
        "    conv6_2_mbox_loc = Conv2D(n_boxes[2] * 8, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2_mbox_loc')(conv6_2)\n",
        "    conv7_2_mbox_loc = Conv2D(n_boxes[3] * 8, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2_mbox_loc')(conv7_2)\n",
        "    conv8_2_mbox_loc = Conv2D(n_boxes[4] * 8, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_2_mbox_loc')(conv8_2)\n",
        "    conv9_2_mbox_loc = Conv2D(n_boxes[5] * 8, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_2_mbox_loc')(conv9_2)\n",
        "\n",
        "    ### Generate the anchor boxes (called \"priors\" in the original Caffe/C++ implementation, so I'll keep their layer names)\n",
        "\n",
        "    # Output shape of anchors: `(batch, height, width, n_boxes, 8)`\n",
        "    conv4_3_norm_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[0], next_scale=scales[1], aspect_ratios=aspect_ratios[0],\n",
        "                                             two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[0], this_offsets=offsets[0], clip_boxes=clip_boxes,\n",
        "                                             variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv4_3_norm_mbox_priorbox')(conv4_3_norm_mbox_loc)\n",
        "    fc7_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[1], next_scale=scales[2], aspect_ratios=aspect_ratios[1],\n",
        "                                    two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[1], this_offsets=offsets[1], clip_boxes=clip_boxes,\n",
        "                                    variances=variances, coords=coords, normalize_coords=normalize_coords, name='fc7_mbox_priorbox')(fc7_mbox_loc)\n",
        "    conv6_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[2], next_scale=scales[3], aspect_ratios=aspect_ratios[2],\n",
        "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[2], this_offsets=offsets[2], clip_boxes=clip_boxes,\n",
        "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv6_2_mbox_priorbox')(conv6_2_mbox_loc)\n",
        "    conv7_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[3], next_scale=scales[4], aspect_ratios=aspect_ratios[3],\n",
        "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[3], this_offsets=offsets[3], clip_boxes=clip_boxes,\n",
        "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv7_2_mbox_priorbox')(conv7_2_mbox_loc)\n",
        "    conv8_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[4], next_scale=scales[5], aspect_ratios=aspect_ratios[4],\n",
        "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[4], this_offsets=offsets[4], clip_boxes=clip_boxes,\n",
        "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv8_2_mbox_priorbox')(conv8_2_mbox_loc)\n",
        "    conv9_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[5], next_scale=scales[6], aspect_ratios=aspect_ratios[5],\n",
        "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[5], this_offsets=offsets[5], clip_boxes=clip_boxes,\n",
        "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv9_2_mbox_priorbox')(conv9_2_mbox_loc)\n",
        "\n",
        "    ### Reshape\n",
        "\n",
        "    # Reshape the class predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, n_classes)`\n",
        "    # We want the classes isolated in the last axis to perform softmax on them\n",
        "    conv4_3_norm_mbox_conf_reshape = Reshape((-1, n_classes), name='conv4_3_norm_mbox_conf_reshape')(conv4_3_norm_mbox_conf)\n",
        "    fc7_mbox_conf_reshape = Reshape((-1, n_classes), name='fc7_mbox_conf_reshape')(fc7_mbox_conf)\n",
        "    conv6_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv6_2_mbox_conf_reshape')(conv6_2_mbox_conf)\n",
        "    conv7_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv7_2_mbox_conf_reshape')(conv7_2_mbox_conf)\n",
        "    conv8_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv8_2_mbox_conf_reshape')(conv8_2_mbox_conf)\n",
        "    conv9_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv9_2_mbox_conf_reshape')(conv9_2_mbox_conf)\n",
        "\n",
        "    # Reshape the box predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, 4)`\n",
        "    # We want the four box coordinates isolated in the last axis to compute the smooth L1 loss\n",
        "    conv4_3_norm_mbox_loc_reshape = Reshape((-1, 8), name='conv4_3_norm_mbox_loc_reshape')(conv4_3_norm_mbox_loc)\n",
        "    fc7_mbox_loc_reshape = Reshape((-1, 8), name='fc7_mbox_loc_reshape')(fc7_mbox_loc)\n",
        "    conv6_2_mbox_loc_reshape = Reshape((-1, 8), name='conv6_2_mbox_loc_reshape')(conv6_2_mbox_loc)\n",
        "    conv7_2_mbox_loc_reshape = Reshape((-1, 8), name='conv7_2_mbox_loc_reshape')(conv7_2_mbox_loc)\n",
        "    conv8_2_mbox_loc_reshape = Reshape((-1, 8), name='conv8_2_mbox_loc_reshape')(conv8_2_mbox_loc)\n",
        "    conv9_2_mbox_loc_reshape = Reshape((-1, 8), name='conv9_2_mbox_loc_reshape')(conv9_2_mbox_loc)\n",
        "\n",
        "    # Reshape the anchor box tensors, yielding 3D tensors of shape `(batch, height * width * n_boxes, 8)`\n",
        "    conv4_3_norm_mbox_priorbox_reshape = Reshape((-1, 16), name='conv4_3_norm_mbox_priorbox_reshape')(conv4_3_norm_mbox_priorbox)\n",
        "    fc7_mbox_priorbox_reshape = Reshape((-1, 16), name='fc7_mbox_priorbox_reshape')(fc7_mbox_priorbox)\n",
        "    conv6_2_mbox_priorbox_reshape = Reshape((-1, 16), name='conv6_2_mbox_priorbox_reshape')(conv6_2_mbox_priorbox)\n",
        "    conv7_2_mbox_priorbox_reshape = Reshape((-1, 16), name='conv7_2_mbox_priorbox_reshape')(conv7_2_mbox_priorbox)\n",
        "    conv8_2_mbox_priorbox_reshape = Reshape((-1, 16), name='conv8_2_mbox_priorbox_reshape')(conv8_2_mbox_priorbox)\n",
        "    conv9_2_mbox_priorbox_reshape = Reshape((-1, 16), name='conv9_2_mbox_priorbox_reshape')(conv9_2_mbox_priorbox)\n",
        "\n",
        "    ### Concatenate the predictions from the different layers\n",
        "\n",
        "    # Axis 0 (batch) and axis 2 (n_classes or 4, respectively) are identical for all layer predictions,\n",
        "    # so we want to concatenate along axis 1, the number of boxes per layer\n",
        "    # Output shape of `mbox_conf`: (batch, n_boxes_total, n_classes)\n",
        "    mbox_conf = Concatenate(axis=1, name='mbox_conf')([conv4_3_norm_mbox_conf_reshape,\n",
        "                                                       fc7_mbox_conf_reshape,\n",
        "                                                       conv6_2_mbox_conf_reshape,\n",
        "                                                       conv7_2_mbox_conf_reshape,\n",
        "                                                       conv8_2_mbox_conf_reshape,\n",
        "                                                       conv9_2_mbox_conf_reshape])\n",
        "\n",
        "    # Output shape of `mbox_loc`: (batch, n_boxes_total, 4)\n",
        "    mbox_loc = Concatenate(axis=1, name='mbox_loc')([conv4_3_norm_mbox_loc_reshape,\n",
        "                                                     fc7_mbox_loc_reshape,\n",
        "                                                     conv6_2_mbox_loc_reshape,\n",
        "                                                     conv7_2_mbox_loc_reshape,\n",
        "                                                     conv8_2_mbox_loc_reshape,\n",
        "                                                     conv9_2_mbox_loc_reshape])\n",
        "\n",
        "    # Output shape of `mbox_priorbox`: (batch, n_boxes_total, 8)\n",
        "    mbox_priorbox = Concatenate(axis=1, name='mbox_priorbox')([conv4_3_norm_mbox_priorbox_reshape,\n",
        "                                                               fc7_mbox_priorbox_reshape,\n",
        "                                                               conv6_2_mbox_priorbox_reshape,\n",
        "                                                               conv7_2_mbox_priorbox_reshape,\n",
        "                                                               conv8_2_mbox_priorbox_reshape,\n",
        "                                                               conv9_2_mbox_priorbox_reshape])\n",
        "\n",
        "    # The box coordinate predictions will go into the loss function just the way they are,\n",
        "    # but for the class predictions, we'll apply a softmax activation layer first\n",
        "    mbox_conf_softmax = Activation('softmax', name='mbox_conf_softmax')(mbox_conf)\n",
        "\n",
        "    # Concatenate the class and box predictions and the anchors to one large predictions vector\n",
        "    # Output shape of `predictions`: (batch, n_boxes_total, n_classes + 4 + 8)\n",
        "    predictions = Concatenate(axis=2, name='predictions')([mbox_conf_softmax, mbox_loc, mbox_priorbox])\n",
        "\n",
        "    if mode == 'training':\n",
        "        model = Model(inputs=x, outputs=predictions)\n",
        "    elif mode == 'inference':\n",
        "        decoded_predictions = DecodeDetections(confidence_thresh=confidence_thresh,\n",
        "                                               iou_threshold=iou_threshold,\n",
        "                                               top_k=top_k,\n",
        "                                               nms_max_output_size=nms_max_output_size,\n",
        "                                               coords=coords,\n",
        "                                               normalize_coords=normalize_coords,\n",
        "                                               img_height=img_height,\n",
        "                                               img_width=img_width,\n",
        "                                               name='decoded_predictions')(predictions)\n",
        "        model = Model(inputs=x, outputs=decoded_predictions)\n",
        "    elif mode == 'inference_fast':\n",
        "        decoded_predictions = DecodeDetectionsFast(confidence_thresh=confidence_thresh,\n",
        "                                                   iou_threshold=iou_threshold,\n",
        "                                                   top_k=top_k,\n",
        "                                                   nms_max_output_size=nms_max_output_size,\n",
        "                                                   coords=coords,\n",
        "                                                   normalize_coords=normalize_coords,\n",
        "                                                   img_height=img_height,\n",
        "                                                   img_width=img_width,\n",
        "                                                   name='decoded_predictions')(predictions)\n",
        "        model = Model(inputs=x, outputs=decoded_predictions)\n",
        "    else:\n",
        "        raise ValueError(\"`mode` must be one of 'training', 'inference' or 'inference_fast', but received '{}'.\".format(mode))\n",
        "\n",
        "    if return_predictor_sizes:\n",
        "        predictor_sizes = np.array([conv4_3_norm_mbox_conf.shape[1:3],\n",
        "                                     fc7_mbox_conf.shape[1:3],\n",
        "                                     conv6_2_mbox_conf.shape[1:3],\n",
        "                                     conv7_2_mbox_conf.shape[1:3],\n",
        "                                     conv8_2_mbox_conf.shape[1:3],\n",
        "                                     conv9_2_mbox_conf.shape[1:3]])\n",
        "        return model, predictor_sizes\n",
        "    else:\n",
        "        return model\n"
      ],
      "metadata": {
        "id": "IRTHFSBsmqTV"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ssd = ssd_300((512,512,3),\n",
        "            1,\n",
        "            mode='inference',\n",
        "            l2_regularization=0.0005,\n",
        "            min_scale=0.01,\n",
        "            max_scale=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JImhguufs2v1",
        "outputId": "0f438cf4-1348-42a9-cbbd-ba971a525563"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grid_shape:  (64, 64)\n",
            "grid_shape:  (64, 64, 1)\n",
            "grid_shape:  (32, 32)\n",
            "grid_shape:  (32, 32, 1)\n",
            "grid_shape:  (16, 16)\n",
            "grid_shape:  (16, 16, 1)\n",
            "grid_shape:  (8, 8)\n",
            "grid_shape:  (8, 8, 1)\n",
            "grid_shape:  (6, 6)\n",
            "grid_shape:  (6, 6, 1)\n",
            "grid_shape:  (4, 4)\n",
            "grid_shape:  (4, 4, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ssd = ssd_300((512,512,3),\n",
        "            1,\n",
        "            mode='inference',\n",
        "            l2_regularization=0.0005,\n",
        "            min_scale=1,\n",
        "            max_scale=1)\n",
        "\n",
        "\n",
        "imarray = np.random.rand(1,512,512,3) * 255\n",
        "\n",
        "pred = ssd.predict(imarray)\n",
        "print(pred)"
      ],
      "metadata": {
        "id": "UiRolxai8rQX",
        "outputId": "ea53f24f-0f5b-4798-9178-348d0b13ff7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grid_shape:  (64, 64)\n",
            "grid_shape:  (64, 64, 1)\n",
            "grid_shape:  (32, 32)\n",
            "grid_shape:  (32, 32, 1)\n",
            "grid_shape:  (16, 16)\n",
            "grid_shape:  (16, 16, 1)\n",
            "grid_shape:  (8, 8)\n",
            "grid_shape:  (8, 8, 1)\n",
            "grid_shape:  (6, 6)\n",
            "grid_shape:  (6, 6, 1)\n",
            "grid_shape:  (4, 4)\n",
            "grid_shape:  (4, 4, 1)\n",
            "grid_shape:  (4, 4)\n",
            "grid_shape:  (4, 4, 1)\n",
            "grid_shape:  (6, 6)\n",
            "grid_shape:  (6, 6, 1)\n",
            "grid_shape:  (8, 8)\n",
            "grid_shape:  (8, 8, 1)\n",
            "grid_shape:  (16, 16)\n",
            "grid_shape:  (16, 16, 1)\n",
            "grid_shape:  (32, 32)\n",
            "grid_shape:  (32, 32, 1)\n",
            "grid_shape:  (64, 64)\n",
            "grid_shape:  (64, 64, 1)\n",
            "[[[ 6.0000000e+00  6.9675421e+02 -7.7179718e-01 -4.6094513e-01\n",
            "    1.2045180e+02  1.1630095e+02]\n",
            "  [ 2.0000000e+00  6.8248212e+02 -2.4285355e+00 -2.4285355e+00\n",
            "    1.3482852e+02  1.3482852e+02]\n",
            "  [ 5.0000000e+00  6.7623706e+02 -7.3547363e-02 -2.2583008e-03\n",
            "    1.0791355e+02  1.0336226e+02]\n",
            "  ...\n",
            "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "    0.0000000e+00  0.0000000e+00]\n",
            "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "    0.0000000e+00  0.0000000e+00]\n",
            "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "    0.0000000e+00  0.0000000e+00]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xP5PUq9f8cI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble"
      ],
      "metadata": {
        "id": "xJSTSiC62bsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "def HackersEnsemble(image_size, num_classes):\n",
        "\n",
        "  input = keras.Input(shape=(image_size, image_size, 3))\n",
        "  \n",
        "  models = [\n",
        "            DeeplabV3Plus(image_size=image_size, num_classes=num_classes, model_input=input)\n",
        "  ]\n",
        "\n",
        "  x = models[0].layers[-1].output\n",
        "\n",
        "  for i, in range(1,len(models)):\n",
        "    x = models[i].layers[-1](x)\n",
        "\n",
        "  merged = Model(inputs=[input],outputs=[x])\n",
        "\n",
        "  return merged"
      ],
      "metadata": {
        "id": "QZPqCDTa2ffh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "ensemble_model = HackersEnsemble(256, 1)\n",
        "plot_model(ensemble_model, to_file='demo.png',show_shapes=True)\n",
        "\n",
        "from IPython.display import Image\n",
        "Image('demo.png')"
      ],
      "metadata": {
        "id": "pKdumDRvxEa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline\n",
        "\n",
        "1. Fetch the Validation Data\n",
        "2. Initialize the model\n",
        "3. Start model evaluation"
      ],
      "metadata": {
        "id": "_BwEZo19xwSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch the Validation Data\n",
        "\n",
        "We need to get the validation data, unzip it plus the labels and prepare a Dataloader for feeding it to the network, that will be initialized in the next step"
      ],
      "metadata": {
        "id": "2paCAv42yJgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for fetching the validation data\n",
        "\n",
        "# Make sure the data is present in the right directory\n",
        "if os.path.exists(\"unzipped_images/val/images\") == False:\n",
        "  prepare_validation_data()"
      ],
      "metadata": {
        "id": "co6E_baJyCaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for creating the data loader\n",
        "\n"
      ],
      "metadata": {
        "id": "HkT0o6CVCF4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the model\n",
        "\n",
        "The model will be trained by that point so we just need to load it"
      ],
      "metadata": {
        "id": "rw1ZnICMyUrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for initializing the model\n",
        "ensemble = HackersEnsemble(256, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "gm_C8hduyTGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start the evaluation\n",
        "\n",
        "The evaluation code should assign the data to the model. And apply the evaluation criteria to it.\n",
        "\n",
        "1. Measure the inference\n",
        "2. measure the IoU"
      ],
      "metadata": {
        "id": "7IqE6vgPydql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "tIXjoNbtO3RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This part is just to test if an image can be passed, this should work\n",
        "def pipeline_working() -> bool:\n",
        "  try:\n",
        "    # get an image\n",
        "    img = Image.open(\"unzipped_images/val/images/15208.png\")\n",
        "    img_resized = resize(img, 256)\n",
        "    array = tf.keras.preprocessing.image.img_to_array(img_resized)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "\n",
        "    # Forward pass\n",
        "    mask = ensemble(array)\n",
        "    mask = mask.numpy()\n",
        "    mask = mask.squeeze(0)\n",
        "    mask = cv2.normalize(mask, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
        "\n",
        "\n",
        "    display(img_resized)\n",
        "    cv2_imshow(mask)\n",
        "\n",
        "    return True\n",
        "  \n",
        "  except Exception as e:\n",
        "    print(\"test failed because of \", e)\n",
        "    return False\n",
        "\n",
        "\n",
        "print(\"The pipeline passed the test: \", pipeline_working())"
      ],
      "metadata": {
        "id": "5yvFZUgRQY3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for starting the evaluation\n",
        "\n",
        "batch_size = 4\n",
        "directory = \"unzipped_images/val/images\"\n",
        "\n",
        "filenames = os.listdir(directory)\n",
        "max_iterations = len(filenames)\n",
        "max_iterations = 5*batch_size #TODO: FOR testing reduce the number throughputs\n",
        "\n",
        "for i in range(0,max_iterations, batch_size):\n",
        "\n",
        "  img = Image.open(os.path.join(directory,filenames[i]))\n",
        "  img_resized = resize(img, IMAGE_SIZE)\n",
        "  array = tf.keras.preprocessing.image.img_to_array(img_resized)\n",
        "  array = np.expand_dims(array, axis=0)\n",
        "\n",
        "  for j in range(1,batch_size):\n",
        "    new_img = Image.open(os.path.join(directory,filenames[i+j]))\n",
        "    new_img_resized = resize(new_img, IMAGE_SIZE)\n",
        "    new_array = tf.keras.preprocessing.image.img_to_array(new_img_resized)\n",
        "    new_array = np.expand_dims(new_array, axis=0)\n",
        "\n",
        "    array = np.concatenate([array, new_array])\n",
        "\n",
        "\n",
        "\n",
        "  # Forward pass\n",
        "  # TODO: This won't be a mask but 8 points later when the ssd is introduced into the ensemble\n",
        "  mask_output = ensemble(array)\n",
        "  mask_output = mask_output.numpy()\n",
        "\n",
        "  for j in range(mask_output.shape[0]):\n",
        "    mask = mask_output[j]\n",
        "    mask = cv2.normalize(mask, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
        "    cv2_imshow(mask)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M2IJgWKyycsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "H3sYwKxfNfV3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}